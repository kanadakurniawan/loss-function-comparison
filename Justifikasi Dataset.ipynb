{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ojBEDhx4FktFvvF9_6EBmn1yKkYppJ2h"},"id":"9XOrVLySmXRX","outputId":"d527c694-1cdc-4bf4-8d13-6f8578559be8","executionInfo":{"status":"ok","timestamp":1746083754961,"user_tz":-420,"elapsed":1305568,"user":{"displayName":"kanada kurniawan","userId":"06806672431675683595"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# ==============================================================================\n","# KODE GABUNGAN UNTUK EKSPLORASI DATASET TESIS (VERSI FINAL + SOLAR POWER)\n","# ==============================================================================\n","# Perubahan: Dataset 7 diganti dengan Solar Power Generation.\n","#            Parser CSV dikembalikan ke versi generik (tanpa filter store/item).\n","# Perbaikan sebelumnya dipertahankan (encoding, subsetting data panjang).\n","#\n","# Harap baca komentar \"Penting\" di bawah sebelum menjalankan.\n","# Sesuaikan `selected_series_...` untuk M3, M4 Daily, M4 Hourly, Tourism jika perlu.\n","# !! PENTING !! Uncomment bagian Google Drive Mount jika ingin menganalisis\n","#                M4 Daily (Dataset 4) atau Solar Power (Dataset 7).\n","# !! PENTING !! Verifikasi NAMA FILE dan NAMA KOLOM ('DATE_TIME', 'DC_POWER')\n","#                untuk Dataset 7 di DATASET_CONFIG[7] sesuai file Anda!\n","\n","# --- Instalasi Library ---\n","# !pip install neuralforecast statsmodels requests pandas numpy matplotlib seaborn -q\n","# !pip install tsfresh # Opsional, jika ingin fitur ekstraksi time series\n","# !pip install codecarbon # Opsional, jika ingin melacak emisi nanti (tidak digunakan di sini)\n","# print(\"Instalasi library eksternal selesai.\")\n","\n","# --- Import Libraries Utama ---\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf # Untuk analisis autokorelasi\n","from scipy.stats import describe # Statistik deskriptif lebih detail\n","import requests\n","from io import StringIO\n","import os\n","import traceback\n","import random\n","import torch # Meskipun tidak training, perlu untuk seed jika ada\n","import warnings\n","\n","warnings.filterwarnings('ignore', category=FutureWarning)\n","warnings.filterwarnings('ignore', category=UserWarning)\n","\n","print(\"Library Python berhasil diimpor.\")\n","\n","# --- Konfigurasi & Seed ---\n","seed = 42\n","np.random.seed(seed)\n","random.seed(seed)\n","# torch.manual_seed(seed) # Kurang relevan untuk eksplorasi saja\n","\n","DATASET_CONFIG = {\n","    1: {\n","        'name': 'Australian Electricity Demand', 'type': 'tsf',\n","        'source': \"https://raw.githubusercontent.com/kanadakurniawan/loss-function-comparison/5b204ef45db85a9ff4e283dd74941dbc117ad287/dataset/australian_electricity_demand_dataset.tsf\",\n","        'freq': '30min', 'parser_variant': 'australia', 'value_column': None, 'time_column': None,\n","        'justification_points': [\"Frekuensi tinggi (30min)\", \"Domain energi\", \"Musiman ganda kuat (harian, mingguan)\", \"Potensi outlier minor\"],\n","        'series_to_select_name_hint': 'T1_NSW',\n","    },\n","    2: {\n","        'name': 'Bike Sharing Daily', 'type': 'csv',\n","        'source': 'https://raw.githubusercontent.com/kanadakurniawan/loss-function-comparison/e2e72b49171794fbc1f285f3eaceff32a8faa5e8/dataset/bike_sharing_dataset.csv',\n","        'freq': 'D', 'parser_variant': None, 'value_column': 'cnt', 'time_column': 'dteday',\n","        'justification_points': [\"Frekuensi sedang (harian)\", \"Domain transportasi\", \"Volatilitas tinggi (pengaruh eksternal)\", \"Potensi outlier/perubahan level\", \"Benchmark permintaan harian\"],\n","        'series_to_select_name_hint': 'Gunakan indeks 0',\n","    },\n","    3: {\n","        'name': 'M3 Monthly', 'type': 'tsf',\n","        'source': 'https://raw.githubusercontent.com/kanadakurniawan/loss-function-comparison/e2e72b49171794fbc1f285f3eaceff32a8faa5e8/dataset/m3_monthly_dataset.tsf',\n","        'freq': 'MS', 'parser_variant': 'standard', 'value_column': None, 'time_column': None,\n","        'justification_points': [\"Frekuensi rendah (bulanan)\", \"Domain Industri/Demografi (pilih seri representatif)\", \"Benchmark klasik bulanan\", \"Variasi panjang, trend, musiman tahunan\"],\n","        'series_to_select_name_hint': 'N1005 (Contoh - verifikasi ID/Indeks!)',\n","    },\n","    4: {\n","        'name': 'M4 Daily', 'type': 'tsf',\n","        'source': '/content/drive/My Drive/S2/Thesis/loss-function-comparison/dataset/m4_daily_dataset.tsf', # Perlu Google Drive Mount!\n","        'freq': 'D', 'parser_variant': 'standard', 'value_column': None, 'time_column': None,\n","        'justification_points': [\"Frekuensi sedang (harian)\", \"Domain Finansial/Ekonomi (pilih seri representatif)\", \"Benchmark M4 Harian\", \"Potensi noise/volatilitas tinggi\", \"Pola kurang jelas/kompleks\"],\n","        'series_to_select_name_hint': 'D412 (Contoh - verifikasi ID/Indeks!)',\n","    },\n","     5: {\n","        'name': 'M4 Hourly', 'type': 'tsf',\n","        'source': 'https://raw.githubusercontent.com/kanadakurniawan/loss-function-comparison/e2e72b49171794fbc1f285f3eaceff32a8faa5e8/dataset/m4_hourly_dataset.tsf',\n","        'freq': 'H', 'parser_variant': 'standard', 'value_column': None, 'time_column': None,\n","        'justification_points': [\"Frekuensi tinggi (jam)\", \"Domain campuran (benchmark M4)\", \"Kontras domain dengan Energi (NSW)\", \"Potensi variabilitas/noise tinggi\"],\n","        'series_to_select_name_hint': 'H1 (Contoh - verifikasi ID/Indeks!)',\n","    },\n","    6: {\n","        'name': 'Tourism Monthly', 'type': 'tsf',\n","        'source': 'https://raw.githubusercontent.com/kanadakurniawan/loss-function-comparison/e2e72b49171794fbc1f285f3eaceff32a8faa5e8/dataset/tourism_monthly_dataset.tsf',\n","        'freq': 'MS', 'parser_variant': 'standard', 'value_column': None, 'time_column': None,\n","        'justification_points': [\"Frekuensi rendah (bulanan)\", \"Domain Pariwisata\", \"Benchmark Tourism Competition\", \"Musiman tahunan biasanya kuat\", \"Rentan guncangan/outlier\"],\n","        'series_to_select_name_hint': 'T1 (Contoh - verifikasi ID/Indeks!)',\n","    },\n","    # --- DATASET 7 DIGANTI ---\n","    7: {\n","        'name': 'Solar Power Generation', # Nama baru\n","        'type': 'csv',\n","        'source': '/content/drive/My Drive/S2/Thesis/loss-function-comparison/dataset/solar_power_generation_dataset_1.csv', # Path baru - Perlu Google Drive Mount!\n","        'freq': '15min', # Frekuensi data solar (per 15 menit)\n","        'parser_variant': None,\n","        'value_column': 'DC_POWER', # Kolom nilai baru - !! VERIFIKASI NAMA INI !!\n","        'time_column': 'DATE_TIME',  # Kolom waktu baru - !! VERIFIKASI NAMA INI !!\n","        'justification_points': [\n","            \"Frekuensi tinggi (15min)\",\n","            \"Domain Energi (Terbarukan)\",\n","            \"Intermittency tinggi (nilai nol saat malam)\", # Alasan utama penggantian\n","            \"Pola harian kuat (siklus diurnal)\",\n","            \"Pengaruh cuaca (noise/volatilitas)\",\n","            \"Potensi anomali sensor/outlier\"\n","        ],\n","        'series_to_select_name_hint': 'Gunakan indeks 0 (CSV dibaca sbg 1 series)',\n","    }\n","    # --------------------------\n","}\n","NAN_IMPUTATION_METHOD = 'ffill_bfill'\n","os.environ['NIXTLA_ID_AS_COL'] = '1'\n","\n","print(\"Konfigurasi dataset (Dataset 7 diganti dgn Solar Power) dimuat.\")\n","\n","# --- Fungsi Helper (Loading, Parsing, Cleaning, Preparing) ---\n","\n","def load_data_from_source(source_path_or_url):\n","    \"\"\"Membaca konten data dari URL atau file lokal dengan fallback encoding.\"\"\"\n","    source_str = str(source_path_or_url)\n","    if source_str.startswith('http'):\n","        print(f\"Mengunduh data dari: {source_str}\")\n","        try:\n","            response = requests.get(source_str, timeout=60)\n","            response.raise_for_status()\n","            print(\"Data berhasil diunduh.\")\n","            try:\n","                encoding = response.encoding if response.encoding else 'utf-8'\n","                print(f\"  Mencoba decode dengan encoding terdeteksi/default: {encoding}\")\n","                return response.content.decode(encoding)\n","            except UnicodeDecodeError as e_utf:\n","                print(f\"  Gagal decode dengan {encoding}: {e_utf}. Mencoba iso-8859-1...\")\n","                try:\n","                    decoded_content = response.content.decode('iso-8859-1')\n","                    print(\"  Berhasil decode dengan iso-8859-1.\")\n","                    return decoded_content\n","                except UnicodeDecodeError as e_latin1:\n","                    print(f\"  Gagal decode dengan iso-8859-1 juga: {e_latin1}\")\n","                    print(\"Error: Gagal decode konten unduhan dgn encoding yg dicoba.\"); return None\n","        except requests.exceptions.RequestException as e: print(f\"Error mengunduh data: {e}\"); return None\n","        except Exception as e: print(f\"Error lain saat mengunduh/proses awal: {e}\"); return None\n","    else:\n","        print(f\"Membaca data dari file lokal: {source_str}\")\n","        try:\n","            if not os.path.exists(source_str):\n","                print(f\"Error: File tdk ditemukan di {source_str}\")\n","                if 'drive/My Drive' in source_str: print(\"Tips: Pastikan Google Drive ter-mount.\"); return None\n","            encodings_to_try = ['utf-8', 'iso-8859-1', 'latin1']\n","            content = None; last_exception = None\n","            for enc in encodings_to_try:\n","                try:\n","                    with open(source_str, 'r', encoding=enc) as f: content = f.read()\n","                    print(f\"Data berhasil dibaca dgn encoding '{enc}'.\"); break\n","                except UnicodeDecodeError: print(f\"Gagal baca dgn enc '{enc}'.\"); last_exception = f\"Gagal decode dgn {enc}\"\n","                except Exception as e: print(f\"Error baca file {source_str} dgn enc '{enc}': {e}\"); last_exception = e; break # Stop jika error lain\n","            if content is None: print(f\"Error: Gagal baca file dgn enc yg dicoba. Err terakhir: {last_exception}\"); return None\n","            return content\n","        except Exception as e: print(f\"Error tak terduga baca file lokal {source_str}: {e}\"); return None\n","\n","def parse_tsf_data(raw_content, parser_variant):\n","    \"\"\"Mem-parsing data TSF mentah (dari string).\"\"\"\n","    parsed_series, series_ids, start_times = [], [], []\n","    print(f\"Memulai parsing TSF (varian: {parser_variant})...\")\n","    lines = raw_content.splitlines(); reading_data = False; skipped_lines = 0; parsed_count = 0\n","    for i, line in enumerate(lines):\n","        line = line.strip()\n","        if not line or line.startswith((\"#\", \"@relation\", \"@attribute\", \"@frequency\", \"@horizon\", \"@missing\", \"@equallength\")): continue\n","        if line.startswith(\"@data\"): reading_data = True; continue\n","        if reading_data:\n","            parts = line.split(\":\")\n","            try:\n","                if parser_variant == 'australia' and len(parts) >= 4: series_name, state_name, start_time_str, values_str = parts[0], parts[1], parts[2], parts[3]; unique_id = f\"{series_name}_{state_name}\"\n","                elif parser_variant == 'standard' and len(parts) >= 3: series_name, start_time_str, values_str = parts[0], parts[1], parts[2]; unique_id = series_name\n","                else: skipped_lines += 1; continue\n","                try: start_time = pd.Timestamp(start_time_str.replace(' ', 'T'))\n","                except ValueError:\n","                     try: start_time = pd.to_datetime(start_time_str, format='%Y-%m-%d %H-%M-%S')\n","                     except ValueError: skipped_lines += 1; continue\n","                time_series = []\n","                for val_str in values_str.split(\",\"):\n","                    val_str = val_str.strip()\n","                    if val_str and val_str != '?':\n","                        try: time_series.append(float(val_str))\n","                        except ValueError: time_series.append(np.nan)\n","                    elif val_str == '?': time_series.append(np.nan)\n","                if time_series: parsed_series.append(time_series); series_ids.append(unique_id); start_times.append(start_time); parsed_count += 1\n","                else: skipped_lines += 1\n","            except Exception as e: skipped_lines += 1; pass\n","    if skipped_lines > 0: print(f\"Peringatan: Melewati {skipped_lines} baris/series saat parsing TSF.\")\n","    print(f\"Parsing TSF selesai. {parsed_count} series berhasil diparsing.\")\n","    return series_ids, start_times, parsed_series\n","\n","# !! FUNGSI parse_csv_data DIKEMBALIKAN KE VERSI GENERIK (TANPA FILTER STORE/ITEM) !!\n","def parse_csv_data(raw_content, time_col, value_col, dataset_name):\n","    \"\"\"\n","    Mem-parsing data CSV mentah (dari string).\n","    Mencoba mendeteksi multi-series tapi memproses semua baris sebagai satu.\n","    \"\"\"\n","    print(f\"Memulai parsing CSV (time: '{time_col}', value: '{value_col}')...\")\n","    try:\n","        df = pd.read_csv(StringIO(raw_content))\n","        if time_col not in df.columns: raise ValueError(f\"Kolom waktu '{time_col}' TIDAK DITEMUKAN. Kolom: {df.columns.tolist()}. Periksa config!\")\n","        if value_col not in df.columns: raise ValueError(f\"Kolom nilai '{value_col}' TIDAK DITEMUKAN. Kolom: {df.columns.tolist()}. Periksa config!\")\n","\n","        # --- Pengecekan Multi-Series Generik (Tanpa Filter) ---\n","        potential_id_cols = [col for col in ['store', 'item', 'ID', 'Id', 'PLANT_ID', 'SOURCE_KEY'] if col in df.columns] # Tambah ID potensial\n","        if potential_id_cols:\n","            try:\n","                num_series_in_file = df[potential_id_cols].drop_duplicates().shape[0]\n","                if num_series_in_file > 1:\n","                     print(f\"PERINGATAN: File CSV '{dataset_name}' tampaknya berisi {num_series_in_file} kombinasi unik dari kolom {potential_id_cols}.\")\n","                     print(\"            Kode ini akan memproses SEMUA baris sebagai SATU series tunggal.\")\n","                     print(\"            Jika ingin analisis per series unik, filter data SEBELUM parsing atau modifikasi parser ini.\")\n","            except Exception as e:\n","                print(f\"Info: Gagal menghitung kombinasi unik ID {potential_id_cols}: {e}\")\n","        # ------------------------------------------------------\n","\n","        df[time_col] = pd.to_datetime(df[time_col])\n","        df[value_col] = pd.to_numeric(df[value_col], errors='coerce')\n","        # Urutkan semua baris berdasarkan waktu (penting jika ada beberapa series yang dibaca jadi satu)\n","        df = df.sort_values(by=time_col).reset_index(drop=True)\n","\n","        start_time = df[time_col].iloc[0]\n","        time_series = df[value_col].tolist()\n","        # ID generik untuk series gabungan (jika ada)\n","        unique_id = f\"{dataset_name.replace(' ', '_')}_Series\"\n","\n","        nan_count = df[value_col].isna().sum()\n","        if nan_count > 0: print(f\"Peringatan: {nan_count} NaN di kolom '{value_col}' stlh parsing CSV.\")\n","\n","        print(f\"Parsing CSV selesai. 1 series ('{unique_id}') ditemukan dgn {len(time_series)} data.\")\n","        return [unique_id], [start_time], [time_series]\n","\n","    except Exception as e:\n","        print(f\"Error parsing CSV: {e}\\n{traceback.format_exc()}\")\n","        return [], [], []\n","\n","def load_and_parse_data(dataset_index, config):\n","    \"\"\"Fungsi utama memuat dan mem-parsing dataset.\"\"\"\n","    cfg = config.get(dataset_index);\n","    if not cfg: raise ValueError(f\"Config u/ Indeks Dataset '{dataset_index}' tdk ditemukan.\")\n","    dataset_name = cfg['name']\n","    print(f\"\\n-- Memuat Dataset {dataset_index}: {dataset_name} --\")\n","    raw_content = load_data_from_source(cfg['source'])\n","    if raw_content is None: print(f\"Gagal muat konten u/ '{dataset_name}'. Eksplorasi dibatalkan.\"); return None, None, None, None, None\n","    data_freq = cfg['freq']\n","    if cfg['type'] == 'tsf': ids, starts, series_list = parse_tsf_data(raw_content, cfg['parser_variant'])\n","    elif cfg['type'] == 'csv':\n","        if not cfg.get('time_column') or not cfg.get('value_column'): print(f\"Error: Config CSV '{dataset_name}' kurang time/value col.\"); return None, None, None, None, None\n","        # Akan memanggil parse_csv_data versi generik\n","        ids, starts, series_list = parse_csv_data(raw_content, cfg['time_column'], cfg['value_column'], dataset_name)\n","    else: raise ValueError(f\"Tipe dataset '{cfg['type']}' tdk dikenal u/ '{dataset_name}'.\")\n","    if not ids: print(f\"Tdk ada series diparsing u/ '{dataset_name}'.\"); return None, None, None, None, None\n","    return ids, starts, series_list, data_freq, dataset_name\n","\n","def select_series(all_ids, all_start_times, all_series_data, index_or_name, dataset_name):\n","    \"\"\"Memilih time series spesifik berdasarkan indeks numerik atau nama.\"\"\"\n","    if not all_ids: raise ValueError(f\"Tdk ada data series tersedia u/ dipilih dari '{dataset_name}'.\")\n","    selected_index = -1\n","    if isinstance(index_or_name, int):\n","        if 0 <= index_or_name < len(all_ids): selected_index = index_or_name\n","        else: raise ValueError(f\"Indeks series numerik {index_or_name} tdk valid u/ '{dataset_name}'. Pilih 0-{len(all_ids) - 1}.\")\n","    elif isinstance(index_or_name, str):\n","        try: selected_index = all_ids.index(index_or_name)\n","        except ValueError:\n","            print(f\"Peringatan: Nama series persis '{index_or_name}' tdk ditemukan di ID u/ '{dataset_name}'.\")\n","            print(f\"  ID tersedia (sampai 10): {all_ids[:10]}\")\n","            lower_name = index_or_name.lower()\n","            found_indices = [i for i, id_val in enumerate(all_ids) if id_val.lower() == lower_name]\n","            if len(found_indices) == 1:\n","                selected_index = found_indices[0]; print(f\"  -> Ditemukan match case-insensitive: '{all_ids[selected_index]}'. Menggunakan ini.\")\n","            elif len(found_indices) > 1: raise ValueError(f\"Nama '{index_or_name}' (case-insensitive) cocok >1 ID. Gunakan ID unik/indeks.\")\n","            else: raise ValueError(f\"Nama series '{index_or_name}' (persis/case-insensitive) tdk ditemukan.\")\n","    else: raise TypeError(f\"'index_or_name' hrs int/str. Diberikan: {type(index_or_name)}\")\n","    selected_id = all_ids[selected_index]; start_time = all_start_times[selected_index]; time_series = all_series_data[selected_index]\n","    print(f\"\\nMemilih series: '{selected_id}' (Index: {selected_index}, Diminta: '{index_or_name}') dari '{dataset_name}'\")\n","    if not time_series: print(f\"Peringatan: Time series '{selected_id}' kosong.\"); return selected_id, start_time, []\n","    print(f\"  -> {len(time_series)} titik data, mulai dari {start_time}.\")\n","    return selected_id, start_time, time_series\n","\n","def handle_nan_values(ts, method='ffill_bfill'):\n","    \"\"\"Menangani nilai NaN.\"\"\"\n","    if not isinstance(ts, (list, np.ndarray, pd.Series)): raise TypeError(\"Input 'ts' hrs list/array/Series.\")\n","    if len(ts) == 0: print(\"Warn: TS kosong sblm handle NaN.\"); return []\n","    ts_series = pd.Series(ts, dtype=float); initial_nan_count = ts_series.isna().sum()\n","    if initial_nan_count == 0: print(\"Tidak ada nilai NaN.\"); return ts_series.tolist()\n","    print(f\"Menangani {initial_nan_count}/{len(ts_series)} NaN dgn metode: {method}\")\n","    filled_ts = None\n","    if method == 'ffill_bfill': filled_ts = ts_series.ffill().bfill()\n","    elif method == 'mean': mean_val = ts_series.mean(); fill_value = mean_val if pd.notna(mean_val) else 0; filled_ts = ts_series.fillna(fill_value); print(f\"  Imputasi mean: {fill_value:.4f}\" + (\"\" if pd.notna(mean_val) else \" (rata2 NaN, isi 0)\"))\n","    elif method == 'median': median_val = ts_series.median(); fill_value = median_val if pd.notna(median_val) else 0; filled_ts = ts_series.fillna(fill_value); print(f\"  Imputasi median: {fill_value:.4f}\" + (\"\" if pd.notna(median_val) else \" (median NaN, isi 0)\"))\n","    elif method == 'interpolate_linear': filled_ts = ts_series.interpolate(method='linear', limit_direction='both').ffill().bfill()\n","    else: print(f\"Warn: Metode '{method}' tdk dikenal. Pakai 'ffill_bfill'.\"); filled_ts = ts_series.ffill().bfill()\n","    final_nan_count = filled_ts.isna().sum()\n","    if final_nan_count > 0: print(f\"Warn: Masih ada {final_nan_count} NaN stlh '{method}'. Isi sisa dgn 0.\"); filled_ts = filled_ts.fillna(0)\n","    if initial_nan_count > 0 and final_nan_count == 0: print(\"Semua NaN berhasil ditangani.\")\n","    return filled_ts.tolist()\n","\n","def prepare_dataframe_for_neuralforecast(time_series, unique_id, start_time, freq):\n","    \"\"\"Mempersiapkan DataFrame format NeuralForecast.\"\"\"\n","    if not isinstance(time_series, (list, np.ndarray)): raise TypeError(\"time_series hrs list/array\")\n","    if len(time_series) == 0: raise ValueError(\"time_series kosong\")\n","    if not isinstance(start_time, pd.Timestamp):\n","         try: start_time = pd.Timestamp(start_time)\n","         except Exception as e: raise ValueError(f\"start_time tdk valid: {start_time} - {e}\")\n","    if not freq: raise ValueError(\"freq tdk boleh kosong/None\")\n","    try:\n","        timestamps = pd.date_range(start=start_time, periods=len(time_series), freq=freq)\n","    except (ValueError, pd.errors.OutOfBoundsDatetime, OverflowError) as e:\n","        print(f\"Error saat membuat date_range: start={start_time}, periods={len(time_series)}, freq='{freq}'\")\n","        print(f\"Error detail: {e}\")\n","        print(\"Ini mungkin terjadi jika 'periods' terlalu besar (misal, data CSV multi-series dibaca sbg satu).\")\n","        raise\n","    df = pd.DataFrame({\"ds\": timestamps, \"y\": time_series}); df[\"unique_id\"] = unique_id\n","    df['y'] = df['y'].astype(float); return df\n","\n","# --- Fungsi Tambahan untuk Eksplorasi (plot_time_series_analysis DIMODIFIKASI) ---\n","def plot_time_series_analysis(df, series_name, freq):\n","    \"\"\"\n","    Melakukan plot time series dasar, dekomposisi, ACF, dan PACF.\n","    Jika data > MAX_POINTS_FOR_DETAIL_ANALYSIS, dekomposisi & ACF/PACF dilakukan pada subset terakhir.\n","    \"\"\"\n","    if not isinstance(df.index, pd.DatetimeIndex): print(\"Error: Indeks DataFrame bukan DatetimeIndex.\"); return\n","    y = df['y']\n","    print(f\"\\n--- Analisis Visual Time Series: {series_name} ---\")\n","    print(\"Membuat plot time series keseluruhan...\")\n","    plt.figure(figsize=(15, 6)); plt.plot(df.index, y, label=f'Observed ({series_name})', linewidth=0.8); plt.title(f'Time Series Plot (Keseluruhan): {series_name}'); plt.xlabel('Timestamp'); plt.ylabel('Value'); plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n","    MAX_POINTS_FOR_DETAIL_ANALYSIS = 5 * 365 # Default u/ harian\n","    if freq == '15min': MAX_POINTS_FOR_DETAIL_ANALYSIS = 3 * (52560 * 4) # ~3 thn data 15min\n","    elif freq == '30min': MAX_POINTS_FOR_DETAIL_ANALYSIS = 3 * 52560\n","    elif freq == 'H': MAX_POINTS_FOR_DETAIL_ANALYSIS = 3 * 8760\n","    y_analysis = y; is_subset = False\n","    if len(y) > MAX_POINTS_FOR_DETAIL_ANALYSIS:\n","        print(f\"\\nINFO: Data terlalu panjang ({len(y)}). Analisis detail pd {MAX_POINTS_FOR_DETAIL_ANALYSIS} poin terakhir.\")\n","        y_analysis = y.tail(MAX_POINTS_FOR_DETAIL_ANALYSIS); is_subset = True\n","        plt.figure(figsize=(15, 5)); plt.plot(y_analysis.index, y_analysis, label=f'Subset u/ Analisis ({series_name})', linewidth=1); plt.title(f'Plot Subset Terakhir u/ Analisis Detail: {series_name}'); plt.xlabel('Timestamp'); plt.ylabel('Value'); plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n","    else: print(\"\\nMelakukan analisis detail pada seluruh data.\")\n","    data_to_analyze = y_analysis.dropna()\n","    if data_to_analyze.empty: print(\"Data u/ analisis detail kosong stlh dropna(). Lewati dekomp & ACF/PACF.\"); return\n","    period = None; len_analyze = len(data_to_analyze)\n","    if freq == '15min': period = 96 * 7 if len_analyze >= 2 * (96 * 7) else 96 if len_analyze >= 2 * 96 else None # 96 = 24*4 (harian)\n","    elif freq == '30min': period = 48 * 7 if len_analyze >= 2 * (48 * 7) else 48 if len_analyze >= 2 * 48 else None\n","    elif freq == 'H': period = 24 * 7 if len_analyze >= 2 * (24 * 7) else 24 if len_analyze >= 2 * 24 else None\n","    elif freq == 'D': period = 365 if len_analyze >= 2 * 365 else 7 if len_analyze >= 2 * 7 else None\n","    elif freq in ['M', 'MS']: period = 12 if len_analyze >= 2 * 12 else None\n","    if period and len_analyze > period * 2 :\n","        print(f\"\\nMelakukan dekomposisi (Model Aditif, Periode={period})...\")\n","        try:\n","            decomposition = seasonal_decompose(data_to_analyze, model='additive', period=period, extrapolate_trend='freq')\n","            print(\"  Dekomposisi selesai. Membuat plot...\")\n","            fig, axes = plt.subplots(4, 1, figsize=(15, 10), sharex=True)\n","            decomposition.observed.plot(ax=axes[0], legend=False); axes[0].set_ylabel('Observed')\n","            decomposition.trend.plot(ax=axes[1], legend=False); axes[1].set_ylabel('Trend')\n","            decomposition.seasonal.plot(ax=axes[2], legend=False); axes[2].set_ylabel('Seasonal')\n","            decomposition.resid.plot(ax=axes[3], legend=False); axes[3].set_ylabel('Residual')\n","            plt.suptitle(f'Dekomposisi: {series_name} (Periode: {period}){\" - Subset Terakhir\" if is_subset else \"\"}')\n","            plt.xlabel('Timestamp'); plt.tight_layout(rect=[0, 0.03, 1, 0.97]); plt.show()\n","            print(\"\\n  Statistik Deskriptif Residual:\"); resid_stats = pd.Series(decomposition.resid).dropna().describe()\n","            if not resid_stats.empty: print(resid_stats.to_string()); plt.figure(figsize=(10, 4)); sns.histplot(decomposition.resid.dropna(), kde=True); plt.title(f'Distribusi Residual: {series_name}'); plt.xlabel('Residual Value'); plt.show()\n","            else: print(\"  Tdk ada residual valid.\")\n","        except Exception as e: print(f\"  Gagal dekomposisi: {e}\\n{traceback.format_exc()}\")\n","    else: print(f\"\\n  Dekomposisi musiman dilewati (data {len_analyze} poin, perlu > {period*2 if period else 'N/A'} u/ periode {period}).\")\n","    lags = min(96 * 2, len_analyze//2 - 1) if freq == '15min' else min(40, len_analyze//2 - 1) # Lags lebih banyak u/ 15min\n","    if lags > 5:\n","        print(f\"\\nMelakukan plotting ACF dan PACF (lags={lags})...\")\n","        try:\n","            fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n","            plot_acf(data_to_analyze, lags=lags, ax=axes[0], title=f'ACF: {series_name}{\" - Subset\" if is_subset else \"\"}')\n","            plot_pacf(data_to_analyze, lags=lags, ax=axes[1], method='ywm', title=f'PACF: {series_name}{\" - Subset\" if is_subset else \"\"}')\n","            axes[0].grid(True); axes[1].grid(True); plt.tight_layout(); plt.show()\n","        except Exception as e: print(f\"  Gagal plot ACF/PACF: {e}\")\n","    else: print(f\"\\n  Plot ACF/PACF dilewati (data {len_analyze} poin terlalu pendek u/ {lags} lags).\")\n","\n","def display_statistics_and_distribution(df, series_name):\n","    \"\"\" Menampilkan statistik deskriptif dan plot distribusi. \"\"\"\n","    y = df['y']\n","    print(f\"\\n--- Analisis Statistik & Distribusi: {series_name} ---\")\n","    print(\"\\nStatistik Deskriptif Utama:\"); print(y.describe().to_string())\n","    print(\"\\nStatistik Tambahan:\")\n","    print(f\"  Skewness: {y.skew():.4f}\"); print(f\"  Kurtosis: {y.kurt():.4f}\")\n","    nan_count = y.isna().sum(); print(f\"  Jumlah NaN (setelah imputasi): {nan_count}\")\n","    zero_count = (y == 0).sum(); print(f\"  Jumlah Nilai Nol: {zero_count} ({zero_count / len(y) * 100:.2f}%)\") # Hitung nilai nol\n","    plt.figure(figsize=(12, 5)); sns.histplot(y.dropna(), kde=True, bins=50); plt.title(f'Distribusi Nilai: {series_name}'); plt.xlabel('Value'); plt.ylabel('Frequency/Density'); plt.grid(True, axis='y', alpha=0.5); plt.show()\n","    plt.figure(figsize=(8, 6)); sns.boxplot(y=y.dropna()); plt.title(f'Box Plot Nilai: {series_name}'); plt.ylabel('Value'); plt.grid(True, axis='y', alpha=0.5); plt.show()\n","\n","print(\"Fungsi helper eksplorasi dimuat.\")\n","\n","# --- (Opsional) Mount Google Drive ---\n","# !! PENTING !! Uncomment jika ingin menganalisis M4 Daily (Dataset 4)\n","#               atau Solar Power (Dataset 7) dari Google Drive.\n","# -----------------------------------------------------------------------------\n","from google.colab import drive\n","try:\n","    drive.mount('/content/drive')\n","    print(\"Google Drive berhasil di-mount.\")\n","    m4_daily_path = '/content/drive/My Drive/S2/Thesis/loss-function-comparison/dataset/m4_daily_dataset.tsf' # Verifikasi Path!\n","    if os.path.exists(m4_daily_path): print(f\"File M4 Daily ditemukan di: {m4_daily_path}\")\n","    else: print(f\"PERINGATAN: File M4 Daily TIDAK ditemukan di: {m4_daily_path}.\")\n","    solar_path = '/content/drive/My Drive/S2/Thesis/loss-function-comparison/dataset/solar_power_generation_dataset.csv' # Verifikasi Path!\n","    if os.path.exists(solar_path): print(f\"File Solar Power ditemukan di: {solar_path}\")\n","    else: print(f\"PERINGATAN: File Solar Power TIDAK ditemukan di: {solar_path}.\")\n","except Exception as e: print(f\"Gagal mount Google Drive: {e}\")\n","# -----------------------------------------------------------------------------\n","\n","# --- Fungsi Utama untuk Menjalankan Analisis per Dataset ---\n","def explore_dataset(dataset_index, series_index_or_name):\n","    \"\"\" Fungsi utama untuk memuat, membersihkan, menganalisis, dan memvisualisasikan satu dataset. \"\"\"\n","    config = DATASET_CONFIG.get(dataset_index)\n","    if not config: print(f\"Error: Config u/ dataset index {dataset_index} tdk ditemukan.\"); return\n","    print(f\"\\n{'='*80}\\n Memulai Eksplorasi Dataset {dataset_index}: {config['name']} \\n{'='*80}\")\n","    print(\"Justifikasi Kunci:\"); [print(f\"- {point}\") for point in config.get('justification_points', [])]; print(\"-\" * 30)\n","    all_ids, all_start_times, all_series_data, data_freq, dataset_name = load_and_parse_data(dataset_index, DATASET_CONFIG)\n","    if all_ids is None: return\n","    if config['type'] == 'tsf' and len(all_ids) > 1: print(f\"\\nID Series TSF Tersedia (sampai 10): {all_ids[:10]}...\")\n","    elif config['type'] == 'csv': print(f\"\\nID Series CSV Dihasilkan: {all_ids}\")\n","    print(f\"Mencoba memilih berdasarkan '{series_index_or_name}'...\")\n","    try:\n","        selected_id, dataset_start_time, ts_raw = select_series(all_ids, all_start_times, all_series_data, series_index_or_name, dataset_name)\n","        if not ts_raw: print(f\"Error: Series '{selected_id}' kosong stlh seleksi. Eksplorasi dibatalkan.\"); return\n","    except (ValueError, IndexError, TypeError) as e: print(f\"Error saat memilih series: {e}\"); return\n","    print(f\"\\nMembersihkan data mentah (Metode: {NAN_IMPUTATION_METHOD})...\"); ts_cleaned = handle_nan_values(ts_raw, method=NAN_IMPUTATION_METHOD)\n","    if not ts_cleaned: print(f\"Error: Data kosong stlh cleaning NaN. Eksplorasi dibatalkan.\"); return\n","    print(\"\\nMempersiapkan DataFrame...\");\n","    try:\n","        df = prepare_dataframe_for_neuralforecast(ts_cleaned, selected_id, dataset_start_time, data_freq)\n","        df = df.set_index('ds')\n","        print(f\"DataFrame siap ({len(df)} baris). Contoh:\\n{df.head().to_string()}\")\n","    except Exception as e: print(f\"Error prep DataFrame: {e}\"); return\n","    display_statistics_and_distribution(df, selected_id)\n","    plot_time_series_analysis(df, selected_id, data_freq)\n","    print(f\"\\n{'='*80}\\n Eksplorasi Selesai: {config['name']} (Series: {selected_id})\\n{'='*80}\\n\\n\")\n","\n","print(\"Fungsi `explore_dataset` siap digunakan.\")\n","\n","\n","# ==============================================================================\n","# EKSEKUSI ANALISIS PER DATASET\n","# ==============================================================================\n","# !! PENTING: Untuk TSF (1, 3, 4, 5, 6), ganti CONTOH INDEKS/NAMA jika perlu.\n","# !! PENTING: Untuk CSV (2, 7), gunakan indeks 0 karena parser menghasilkan 1 series.\n","# !! PENTING: Uncomment eksekusi dataset 4 & 7 HANYA jika Drive sudah mount.\n","# !! PENTING: VERIFIKASI NAMA FILE & KOLOM untuk Dataset 7 di config!\n","\n","# --- Dataset 1: Australian Electricity Demand ---\n","selected_series_aus_elec = \"T1_NSW\"\n","explore_dataset(dataset_index=1, series_index_or_name=selected_series_aus_elec)\n","\n","# --- Dataset 2: Bike Sharing Daily ---\n","selected_series_bike = 0\n","explore_dataset(dataset_index=2, series_index_or_name=selected_series_bike)\n","\n","# --- Dataset 3: M3 Monthly ---\n","selected_series_m3 = 0 # <--- CONTOH INDEKS, PERIKSA & GANTI JIKA PERLU\n","explore_dataset(dataset_index=3, series_index_or_name=selected_series_m3)\n","\n","# --- Dataset 4: M4 Daily ---\n","selected_series_m4_daily = 0 # <--- CONTOH INDEKS, PERIKSA & GANTI JIKA PERLU\n","explore_dataset(dataset_index=4, series_index_or_name=selected_series_m4_daily)\n","print(\"\\nEksplorasi Dataset 4 (M4 Daily) - Dikomentari. Uncomment jika Drive mount & path/indeks benar.\\n\")\n","\n","\n","# --- Dataset 5: M4 Hourly ---\n","selected_series_m4_hourly = 0 # <--- CONTOH INDEKS (H1?), PERIKSA & GANTI JIKA PERLU\n","explore_dataset(dataset_index=5, series_index_or_name=selected_series_m4_hourly)\n","\n","# --- Dataset 6: Tourism Monthly ---\n","selected_series_tourism = 0 # <--- CONTOH INDEKS (T1?), PERIKSA & GANTI JIKA PERLU\n","explore_dataset(dataset_index=6, series_index_or_name=selected_series_tourism)\n","\n","# --- Dataset 7: Solar Power Generation ---\n","# Menggunakan indeks 0 karena parser CSV menghasilkan 1 series.\n","# !! PENTING: Pastikan nama kolom 'DATE_TIME' & 'DC_POWER' di config[7] benar!\n","# !! PENTING: Uncomment baris di bawah HANYA JIKA Google Drive sudah di-mount !!\n","selected_series_solar = 0\n","explore_dataset(dataset_index=7, series_index_or_name=selected_series_solar)\n","# print(\"\\nEksplorasi Dataset 7 (Solar Power) - Dikomentari. Uncomment jika Drive mount & path/kolom benar.\\n\")\n","\n","\n","print(\"=== SEMUA EKSEKUSI (YANG TIDAK DIKOMENTARI) SELESAI ===\")"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOJwYEMtYbroplkh/8n+TXa"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}