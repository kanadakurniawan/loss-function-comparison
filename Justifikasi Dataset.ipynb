{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"16MjCORtzpi60hMdRVYmBVYhn-kU7t74y"},"id":"9XOrVLySmXRX","outputId":"47ace2fd-6976-4031-fd7b-6ed751a954c9","executionInfo":{"status":"ok","timestamp":1760709350977,"user_tz":-420,"elapsed":29481,"user":{"displayName":"kanada kurniawan","userId":"06806672431675683595"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# ==============================================================================\n","# KODE GABungan UNTUK EKSPLORASI DATASET TESIS (VERSI FINAL REVISI 8)\n","# ==============================================================================\n","# Perubahan:\n","# - Menambahkan perhitungan jumlah data outlier menggunakan metode IQR\n","#   (Interquartile Range).\n","# - Menambahkan kolom \"Jumlah Outlier\" dan \"% Outlier\" pada tabel\n","#   rangkuman komprehensif di akhir eksekusi.\n","#\n","# Perbaikan sebelumnya dipertahankan.\n","# ==============================================================================\n","\n","# --- Instalasi Library (jika belum ada) ---\n","# !pip install neuralforecast statsmodels requests pandas numpy matplotlib seaborn -q\n","# print(\"Instalasi library eksternal selesai.\")\n","\n","# --- Import Libraries Utama ---\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","from scipy.stats import describe\n","import requests\n","from io import StringIO\n","import os\n","import traceback\n","import random\n","import warnings\n","\n","warnings.filterwarnings('ignore', category=FutureWarning)\n","warnings.filterwarnings('ignore', category=UserWarning)\n","\n","print(\"Library Python berhasil diimpor.\")\n","\n","# --- Konfigurasi & Seed ---\n","seed = 42\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","# --- KONFIGURASI DATASET ---\n","DATASET_CONFIG = {\n","    1: {\n","        'name': 'Bike Sharing', 'type': 'csv',\n","        'source': \"https://raw.githubusercontent.com/kanadakurniawan/loss-function-comparison/8ae1f330d2d94645a6b647ab357fa786a5e1f956/dataset/bike-sharing.csv\",\n","        'freq': 'H', 'parser_variant': None, 'value_column': 'cnt', 'time_column': 'datetime',\n","        'justification_points': [\"Frekuensi tinggi (jam)\", \"Domain transportasi\", \"Musiman ganda (harian, mingguan)\", \"Pengaruh faktor eksternal (cuaca)\"],\n","    },\n","    2: {\n","        'name': 'Pasut BMKG', 'type': 'csv',\n","        'source': 'https://raw.githubusercontent.com/kanadakurniawan/loss-function-comparison/6912fb758bcf4f6984c3d09c70dee9972b987a4b/dataset/db-2022-2024-pasut.csv',\n","        'freq': 'H', 'parser_variant': None, 'value_column': 'pasut', 'time_column': 'datetime',\n","        'justification_points': [\"Frekuensi tinggi (jam)\", \"Domain lingkungan/oseanografi\", \"Pola musiman sangat kuat dan reguler (pasang surut)\", \"Data dunia nyata dari BMKG\"],\n","    },\n","    3: {\n","        'name': 'Parking Birmingham', 'type': 'csv',\n","        'source': 'https://raw.githubusercontent.com/kanadakurniawan/loss-function-comparison/8ae1f330d2d94645a6b647ab357fa786a5e1f956/dataset/parking-birmingham.csv',\n","        'freq': '30min', 'parser_variant': None, 'value_column': 'Occupancy', 'time_column': 'datetime',\n","        'justification_points': [\"Frekuensi tinggi (30 menit)\", \"Domain urban/transportasi\", \"Potensi pola harian dan mingguan yang kompleks\", \"Mengandung nilai nol atau mendekati nol (malam hari)\"],\n","    },\n","    4: {\n","        'name': 'Solar Power Generation', 'type': 'csv',\n","        'source': 'https://raw.githubusercontent.com/kanadakurniawan/loss-function-comparison/8ae1f330d2d94645a6b647ab357fa786a5e1f956/dataset/Actual_31.85_-110.85_2006_UPV_100MW_5_Min.csv',\n","        'freq': '5min', 'parser_variant': None, 'value_column': 'Power(MW)', 'time_column': 'datetime',\n","        'justification_points': [\"Frekuensi sangat tinggi (5 menit)\", \"Domain energi terbarukan\", \"Intermittency tinggi (nilai nol di malam hari)\", \"Sangat dipengaruhi cuaca (volatilitas)\"],\n","    },\n","    5: {\n","        'name': 'Cacar Air Hungaria', 'type': 'csv',\n","        'source': 'https://raw.githubusercontent.com/kanadakurniawan/loss-function-comparison/8ae1f330d2d94645a6b647ab357fa786a5e1f956/dataset/cacar-air-hungaria.csv',\n","        'freq': 'W', 'parser_variant': None, 'value_column': 'BUDAPEST', 'time_column': 'datetime',\n","        'justification_points': [\"Frekuensi rendah (mingguan)\", \"Domain epidemiologi/kesehatan\", \"Musiman tahunan yang jelas\", \"Contoh data count (jumlah kasus)\"],\n","        'notes': \"File ini berisi banyak series (per wilayah). Kode ini akan menganalisis kolom 'BUDAPEST' sebagai contoh.\"\n","    },\n","    6: {\n","        'name': 'M4 Hourly Dataset', 'type': 'tsf',\n","        'source': 'https://raw.githubusercontent.com/kanadakurniawan/loss-function-comparison/8f605f9fbc107e6303174d3f615a5d591785d55e/dataset/m4_hourly_dataset.tsf',\n","        'freq': 'H', 'parser_variant': 'standard', 'value_column': None, 'time_column': None,\n","        'justification_points': [\"Frekuensi tinggi (jam)\", \"Benchmark klasik dari kompetisi M4\", \"Berisi beragam jenis data (industri, finansial, dll.)\", \"Tantangan dalam noise dan variabilitas pola\"],\n","    }\n","}\n","NAN_IMPUTATION_METHOD = 'ffill_bfill'\n","os.environ['NIXTLA_ID_AS_COL'] = '1'\n","\n","print(\"Konfigurasi dataset berhasil diperbarui.\")\n","\n","# --- Fungsi Helper (Loading, Parsing, Cleaning, Preparing) ---\n","\n","def load_data_from_source(source_path_or_url):\n","    source_str = str(source_path_or_url)\n","    if source_str.startswith('http'):\n","        print(f\"Mengunduh data dari: {source_str}\")\n","        try:\n","            response = requests.get(source_str, timeout=60)\n","            response.raise_for_status()\n","            print(\"Data berhasil diunduh.\")\n","            try:\n","                encoding = response.encoding if response.encoding else 'utf-8'\n","                return response.content.decode(encoding)\n","            except UnicodeDecodeError:\n","                return response.content.decode('iso-8859-1')\n","        except requests.exceptions.RequestException as e: print(f\"Error mengunduh data: {e}\"); return None\n","    else:\n","        if not os.path.exists(source_str): print(f\"Error: File tdk ditemukan di {source_str}\"); return None\n","        try:\n","            with open(source_str, 'r', encoding='utf-8') as f: return f.read()\n","        except UnicodeDecodeError:\n","            with open(source_str, 'r', encoding='iso-8859-1') as f: return f.read()\n","        except Exception as e: print(f\"Error membaca file lokal: {e}\"); return None\n","\n","def parse_tsf_data(raw_content, parser_variant):\n","    parsed_series, series_ids, start_times = [], [], []\n","    print(f\"Memulai parsing TSF (varian: {parser_variant})...\")\n","    lines = raw_content.splitlines(); reading_data = False; skipped_lines = 0; parsed_count = 0\n","    for line in lines:\n","        line = line.strip()\n","        if not line or line.startswith((\"#\", \"@relation\", \"@attribute\", \"@frequency\", \"@horizon\", \"@missing\", \"@equallength\")): continue\n","        if line.startswith(\"@data\"): reading_data = True; continue\n","        if reading_data:\n","            parts = line.split(\":\")\n","            try:\n","                if len(parts) < 3: skipped_lines += 1; continue\n","                series_name, start_time_str, values_str = parts[0], parts[1], parts[2]\n","                start_time = pd.to_datetime(start_time_str)\n","                time_series = []\n","                for val_str in values_str.split(\",\"):\n","                    val_str = val_str.strip()\n","                    if val_str and val_str != '?': time_series.append(float(val_str))\n","                    elif val_str == '?': time_series.append(np.nan)\n","                if time_series:\n","                    parsed_series.append(time_series); series_ids.append(series_name); start_times.append(start_time); parsed_count += 1\n","                else: skipped_lines += 1\n","            except Exception: skipped_lines += 1; continue\n","    if skipped_lines > 0: print(f\"Peringatan: Melewati {skipped_lines} baris saat parsing TSF.\")\n","    print(f\"Parsing TSF selesai. {parsed_count} series berhasil diparsing.\")\n","    return series_ids, start_times, parsed_series\n","\n","def parse_csv_data(raw_content, time_col, value_col, dataset_name):\n","    print(f\"Memulai parsing CSV (time: '{time_col}', value: '{value_col}')...\")\n","    try:\n","        df = pd.read_csv(StringIO(raw_content))\n","        if time_col not in df.columns: raise ValueError(f\"Kolom waktu '{time_col}' TIDAK DITEMUKAN. Kolom: {df.columns.tolist()}\")\n","        if value_col not in df.columns: raise ValueError(f\"Kolom nilai '{value_col}' TIDAK DITEMUKAN. Kolom: {df.columns.tolist()}\")\n","        df[time_col] = pd.to_datetime(df[time_col]); df[value_col] = pd.to_numeric(df[value_col], errors='coerce')\n","        df = df.sort_values(by=time_col).reset_index(drop=True)\n","        unique_id = f\"{dataset_name.replace(' ', '_')}_Series\"\n","        return [unique_id], [df[time_col].iloc[0]], [df[value_col].tolist()]\n","    except Exception as e: print(f\"Error parsing CSV: {e}\\n{traceback.format_exc()}\"); return [], [], []\n","\n","def load_and_parse_data(dataset_index, config):\n","    cfg = config.get(dataset_index); dataset_name = cfg['name']\n","    print(f\"\\n-- Memuat Dataset {dataset_index}: {dataset_name} --\")\n","    raw_content = load_data_from_source(cfg['source'])\n","    if raw_content is None: return None, None, None, None, None\n","    if cfg['type'] == 'tsf': return (*parse_tsf_data(raw_content, cfg['parser_variant']), cfg['freq'], dataset_name)\n","    elif cfg['type'] == 'csv': return (*parse_csv_data(raw_content, cfg['time_column'], cfg['value_column'], dataset_name), cfg['freq'], dataset_name)\n","    return None, None, None, None, None\n","\n","def select_series(all_ids, all_start_times, all_series_data, index_or_name, dataset_name):\n","    if not all_ids: raise ValueError(\"Tdk ada data series tersedia u/ dipilih.\")\n","    selected_index = -1\n","    if isinstance(index_or_name, int):\n","        if 0 <= index_or_name < len(all_ids): selected_index = index_or_name\n","        else: raise ValueError(f\"Indeks {index_or_name} di luar jangkauan (0-{len(all_ids) - 1}).\")\n","    elif isinstance(index_or_name, str):\n","        try: selected_index = all_ids.index(index_or_name)\n","        except ValueError: raise ValueError(f\"Nama series '{index_or_name}' tdk ditemukan.\")\n","    else: raise TypeError(\"'index_or_name' hrs int/str.\")\n","    selected_id = all_ids[selected_index]\n","    print(f\"\\nMemilih series: '{selected_id}' (Index: {selected_index}) dari '{dataset_name}'\")\n","    return selected_id, all_start_times[selected_index], all_series_data[selected_index]\n","\n","def handle_nan_values(ts, method='ffill_bfill'):\n","    ts_series = pd.Series(ts, dtype=float)\n","    initial_nan_count = ts_series.isna().sum()\n","    if initial_nan_count == 0:\n","        print(\"Tidak ada nilai NaN.\")\n","        return ts_series.tolist(), initial_nan_count\n","    print(f\"Menangani {initial_nan_count}/{len(ts_series)} NaN dgn metode: {method}\")\n","    if method == 'ffill_bfill': filled_ts = ts_series.ffill().bfill()\n","    else: filled_ts = ts_series.ffill().bfill()\n","    if filled_ts.isna().sum() > 0: filled_ts = filled_ts.fillna(0)\n","    print(\"Semua NaN berhasil ditangani.\")\n","    return filled_ts.tolist(), initial_nan_count\n","\n","def prepare_dataframe_for_neuralforecast(time_series, unique_id, start_time, freq):\n","    timestamps = pd.date_range(start=start_time, periods=len(time_series), freq=freq)\n","    df = pd.DataFrame({\"ds\": timestamps, \"y\": time_series}); df[\"unique_id\"] = unique_id\n","    return df\n","\n","# --- Fungsi Analisis & Visualisasi ---\n","\n","def plot_time_series_analysis(df, series_name, freq):\n","    if not isinstance(df.index, pd.DatetimeIndex): df = df.set_index('ds')\n","    y = df['y']\n","    print(f\"\\n--- Analisis Visual Time Series: {series_name} ---\")\n","    plt.figure(figsize=(15, 6)); plt.plot(y, label=f'Observed ({series_name})', linewidth=0.8); plt.title(f'Time Series Plot: {series_name}'); plt.xlabel('Timestamp'); plt.ylabel('Value'); plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n","    MAX_POINTS_FOR_DETAIL = 3 * 365 * 24\n","    y_analysis = y.tail(MAX_POINTS_FOR_DETAIL) if len(y) > MAX_POINTS_FOR_DETAIL else y\n","    is_subset = len(y) > MAX_POINTS_FOR_DETAIL\n","    if is_subset: print(f\"INFO: Analisis detail dilakukan pada {MAX_POINTS_FOR_DETAIL} poin terakhir.\")\n","    period_map = {'5min': 12*24, '30min': 48, 'H': 24, 'D': 7, 'W': 52, 'MS': 12}\n","    period = period_map.get(freq)\n","    if period and len(y_analysis) > 2 * period:\n","        print(f\"Melakukan dekomposisi (Model Aditif, Periode={period})...\")\n","        try:\n","            decomposition = seasonal_decompose(y_analysis.dropna(), model='additive', period=period)\n","            fig = decomposition.plot(); fig.set_size_inches(15, 10); plt.suptitle(f'Dekomposisi: {series_name}{\" (Subset)\" if is_subset else \"\"}', y=1.01); plt.show()\n","        except Exception as e: print(f\"  Gagal dekomposisi: {e}\")\n","    else: print(\"Dekomposisi musiman dilewati.\")\n","    lags = min(48, len(y_analysis) // 2 - 1)\n","    if lags > 5:\n","        print(f\"Melakukan plotting ACF dan PACF (lags={lags})...\")\n","        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n","        plot_acf(y_analysis.dropna(), lags=lags, ax=axes[0]); plot_pacf(y_analysis.dropna(), lags=lags, ax=axes[1], method='ywm')\n","        plt.suptitle(f'ACF & PACF: {series_name}{\" (Subset)\" if is_subset else \"\"}', y=1.01); plt.show()\n","\n","def calculate_and_display_statistics(df, series_name, initial_nan_count):\n","    y = df['y']\n","    print(f\"\\n--- Analisis Statistik & Distribusi: {series_name} ---\")\n","    desc_stats = y.describe()\n","    print(\"\\nStatistik Deskriptif:\"); print(desc_stats.to_string())\n","    skewness = y.skew(); kurtosis = y.kurt()\n","    zero_count = (y == 0).sum(); zero_pct = zero_count / len(y) * 100\n","\n","    # --- Perhitungan Outlier (Metode IQR) ---\n","    Q1 = y.quantile(0.25)\n","    Q3 = y.quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower_bound = Q1 - 1.5 * IQR\n","    upper_bound = Q3 + 1.5 * IQR\n","    outlier_count = ((y < lower_bound) | (y > upper_bound)).sum()\n","    outlier_pct = outlier_count / len(y) * 100\n","\n","    print(f\"\\n  Skewness: {skewness:.4f}, Kurtosis: {kurtosis:.4f}\")\n","    print(f\"  Jumlah Nilai Nol: {zero_count} ({zero_pct:.2f}%)\")\n","    print(f\"  Jumlah Outlier (IQR): {outlier_count} ({outlier_pct:.2f}%)\")\n","\n","    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n","    sns.histplot(y.dropna(), kde=True, bins=50, ax=axes[0]); axes[0].set_title(f'Distribusi Nilai: {series_name}')\n","    sns.boxplot(y=y.dropna(), ax=axes[1]); axes[1].set_title(f'Box Plot Nilai: {series_name}')\n","    plt.tight_layout(); plt.show()\n","\n","    stats_summary = {\n","        'Observasi': int(desc_stats['count']), 'Mean': desc_stats['mean'], 'Std Dev': desc_stats['std'],\n","        'Min': desc_stats['min'], 'Max': desc_stats['max'], 'Skewness': skewness, 'Kurtosis': kurtosis,\n","        'Nilai Nol': zero_count, '% Nol': zero_pct, 'Missing (Awal)': initial_nan_count,\n","        'Jumlah Outlier': outlier_count, '% Outlier': outlier_pct\n","    }\n","    return stats_summary\n","\n","print(\"Fungsi helper eksplorasi dimuat.\")\n","\n","# --- Fungsi Utama untuk Menjalankan Analisis per Dataset ---\n","def explore_dataset(dataset_index, series_index_or_name=0):\n","    config = DATASET_CONFIG.get(dataset_index)\n","    if not config: print(f\"Error: Config u/ dataset index {dataset_index} tdk ditemukan.\"); return None\n","    print(f\"\\n{'='*80}\\n Memulai Eksplorasi Dataset {dataset_index}: {config['name']} \\n{'='*80}\")\n","    print(\"Justifikasi Kunci:\"); [print(f\"- {point}\") for point in config.get('justification_points', [])]\n","    if 'notes' in config: print(f\"Catatan: {config['notes']}\")\n","    print(\"-\" * 30)\n","    all_ids, all_starts, all_series, freq, name = load_and_parse_data(dataset_index, DATASET_CONFIG)\n","    if not all_ids: return None\n","    try:\n","        selector = series_index_or_name if config['type'] == 'tsf' else 0\n","        selected_id, start_time, ts_raw = select_series(all_ids, all_starts, all_series, selector, name)\n","    except (ValueError, IndexError, TypeError) as e: print(f\"Error saat memilih series: {e}\"); return None\n","    ts_cleaned, initial_nan_count = handle_nan_values(ts_raw, method=NAN_IMPUTATION_METHOD)\n","    if not ts_cleaned: print(\"Error: Data kosong setelah cleaning.\"); return None\n","    try:\n","        df = prepare_dataframe_for_neuralforecast(ts_cleaned, selected_id, start_time, freq)\n","        df = df.set_index('ds')\n","        print(f\"DataFrame siap ({len(df)} baris). Contoh:\\n{df.head().to_string()}\")\n","    except Exception as e: print(f\"Error saat mempersiapkan DataFrame: {e}\"); return None\n","\n","    stats_summary = calculate_and_display_statistics(df, selected_id, initial_nan_count)\n","    plot_time_series_analysis(df, selected_id, freq)\n","\n","    full_summary = {\n","        'Dataset': config['name'], 'Series ID': selected_id,\n","        'Start Date': df.index.min().strftime('%Y-%m-%d'),\n","        'End Date': df.index.max().strftime('%Y-%m-%d'),\n","        'Frekuensi': freq, **stats_summary\n","    }\n","    print(f\"\\n{'='*80}\\n Eksplorasi Selesai: {config['name']} (Series: {selected_id})\\n{'='*80}\\n\\n\")\n","    return full_summary\n","\n","# --- FUNGSI BARU UNTUK MENAMPILKAN RANGKUMAN AKHIR ---\n","def display_final_summary(summaries):\n","    \"\"\"Menampilkan rangkuman akhir dari semua dataset dalam bentuk tabel.\"\"\"\n","    if not summaries:\n","        print(\"Tidak ada data untuk dirangkum.\")\n","        return\n","\n","    df_summary = pd.DataFrame(summaries)\n","\n","    float_cols = ['Mean', 'Std Dev', 'Min', 'Max', 'Skewness', 'Kurtosis']\n","    for col in float_cols:\n","        df_summary[col] = df_summary[col].map('{:,.2f}'.format)\n","    df_summary['% Nol'] = df_summary['% Nol'].map('{:.2f}%'.format)\n","    df_summary['% Outlier'] = df_summary['% Outlier'].map('{:.2f}%'.format)\n","\n","    print(f\"\\n{'='*140}\")\n","    print(f\"{'RANGKUMAN KOMPREHENSIF SEMUA DATASET'.center(140)}\")\n","    print(f\"{'='*140}\")\n","    print(df_summary.to_string())\n","    print(f\"{'='*140}\")\n","\n","# ==============================================================================\n","# EKSEKUSI ANALISIS PER DATASET (VERSI OTOMATIS)\n","# ==============================================================================\n","datasets_to_run = [1, 2, 3, 4, 5, 6]\n","series_selectors = {\n","    6: 0\n","}\n","all_summaries = []\n","\n","# --- Loop Eksekusi ---\n","for index in datasets_to_run:\n","    selector = series_selectors.get(index, 0)\n","    summary = explore_dataset(dataset_index=index, series_index_or_name=selector)\n","    if summary:\n","        all_summaries.append(summary)\n","\n","# --- Tampilkan Rangkuman Akhir ---\n","display_final_summary(all_summaries)\n","\n","print(\"\\n=== SEMUA EKSEKUSI DAN RANGKUMAN TELAH SELESAI ===\")"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMBA1GrNtcNlysuPi7D+tPK"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}