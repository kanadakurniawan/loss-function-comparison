{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":324171,"status":"ok","timestamp":1758516103118,"user":{"displayName":"kanada kurniawan","userId":"06806672431675683595"},"user_tz":-420},"id":"FXPzVhbVr5cH","outputId":"b62f2c24-85db-4fc8-80f9-9f1b15544a4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting neuralforecast\n","  Downloading neuralforecast-3.0.2-py3-none-any.whl.metadata (14 kB)\n","Collecting coreforecast\u003e=0.0.6 (from neuralforecast)\n","  Downloading coreforecast-0.0.16-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from neuralforecast) (2025.3.0)\n","Requirement already satisfied: numpy\u003e=1.21.6 in /usr/local/lib/python3.12/dist-packages (from neuralforecast) (2.0.2)\n","Requirement already satisfied: pandas\u003e=1.3.5 in /usr/local/lib/python3.12/dist-packages (from neuralforecast) (2.2.2)\n","Collecting torch\u003c=2.6.0,\u003e=2.0.0 (from neuralforecast)\n","  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n","Collecting pytorch-lightning\u003e=2.0.0 (from neuralforecast)\n","  Downloading pytorch_lightning-2.5.5-py3-none-any.whl.metadata (20 kB)\n","Collecting ray\u003e=2.2.0 (from ray[tune]\u003e=2.2.0-\u003eneuralforecast)\n","  Downloading ray-2.49.2-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n","Collecting optuna (from neuralforecast)\n","  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n","Collecting utilsforecast\u003e=0.2.3 (from neuralforecast)\n","  Downloading utilsforecast-0.2.12-py3-none-any.whl.metadata (7.6 kB)\n","Requirement already satisfied: python-dateutil\u003e=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas\u003e=1.3.5-\u003eneuralforecast) (2.9.0.post0)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas\u003e=1.3.5-\u003eneuralforecast) (2025.2)\n","Requirement already satisfied: tzdata\u003e=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas\u003e=1.3.5-\u003eneuralforecast) (2025.2)\n","Requirement already satisfied: tqdm\u003e=4.57.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (4.67.1)\n","Requirement already satisfied: PyYAML\u003e5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (6.0.2)\n","Collecting torchmetrics\u003e0.7.0 (from pytorch-lightning\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (25.0)\n","Requirement already satisfied: typing-extensions\u003e4.5.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (4.15.0)\n","Collecting lightning-utilities\u003e=0.10.0 (from pytorch-lightning\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: click\u003e=7.0 in /usr/local/lib/python3.12/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (8.2.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (3.19.1)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (4.25.1)\n","Requirement already satisfied: msgpack\u003c2.0.0,\u003e=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (1.1.1)\n","Requirement already satisfied: protobuf\u003e=3.20.3 in /usr/local/lib/python3.12/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (5.29.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (2.32.4)\n","Collecting tensorboardX\u003e=1.9 (from ray[tune]\u003e=2.2.0-\u003eneuralforecast)\n","  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: pyarrow\u003e=9.0.0 in /usr/local/lib/python3.12/dist-packages (from ray[tune]\u003e=2.2.0-\u003eneuralforecast) (18.1.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparselt-cu12==0.6.2 (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting nvidia-nccl-cu12==2.21.5 (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.4.127 (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting triton==3.2.0 (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast) (75.2.0)\n","Collecting sympy==1.13.1 (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1-\u003etorch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast) (1.3.0)\n","Requirement already satisfied: alembic\u003e=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna-\u003eneuralforecast) (1.16.5)\n","Collecting colorlog (from optuna-\u003eneuralforecast)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: sqlalchemy\u003e=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna-\u003eneuralforecast) (2.0.43)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic\u003e=1.5.0-\u003eoptuna-\u003eneuralforecast) (1.3.10)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (3.12.15)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil\u003e=2.8.2-\u003epandas\u003e=1.3.5-\u003eneuralforecast) (1.17.0)\n","Requirement already satisfied: greenlet\u003e=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy\u003e=1.4.2-\u003eoptuna-\u003eneuralforecast) (3.2.4)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2-\u003etorch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast) (3.0.2)\n","Requirement already satisfied: attrs\u003e=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications\u003e=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (2025.9.1)\n","Requirement already satisfied: referencing\u003e=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (0.36.2)\n","Requirement already satisfied: rpds-py\u003e=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (0.27.1)\n","Requirement already satisfied: charset_normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.12/dist-packages (from requests-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (3.4.3)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (2.5.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (2025.8.3)\n","Requirement already satisfied: aiohappyeyeballs\u003e=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (2.6.1)\n","Requirement already satisfied: aiosignal\u003e=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (1.4.0)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (1.7.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (6.6.4)\n","Requirement already satisfied: propcache\u003e=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (0.3.2)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (1.20.1)\n","Downloading neuralforecast-3.0.2-py3-none-any.whl (260 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coreforecast-0.0.16-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytorch_lightning-2.5.5-py3-none-any.whl (832 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m832.4/832.4 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ray-2.49.2-cp312-cp312-manylinux2014_x86_64.whl (70.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading utilsforecast-0.2.12-py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading optuna-4.5.0-py3-none-any.whl (400 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n","Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Installing collected packages: triton, nvidia-cusparselt-cu12, tensorboardX, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, coreforecast, colorlog, nvidia-cusparse-cu12, nvidia-cudnn-cu12, utilsforecast, optuna, nvidia-cusolver-cu12, torch, ray, torchmetrics, pytorch-lightning, neuralforecast\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.4.0\n","    Uninstalling triton-3.4.0:\n","      Successfully uninstalled triton-3.4.0\n","  Attempting uninstall: nvidia-cusparselt-cu12\n","    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n","    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n","      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n","  Attempting uninstall: sympy\n","    Found existing installation: sympy 1.13.3\n","    Uninstalling sympy-1.13.3:\n","      Successfully uninstalled sympy-1.13.3\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.6.77\n","    Uninstalling nvidia-nvtx-cu12-12.6.77:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n","    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.27.3\n","    Uninstalling nvidia-nccl-cu12-2.27.3:\n","      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.7.77\n","    Uninstalling nvidia-curand-cu12-10.3.7.77:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n","    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n","    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n","    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n","    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n","      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n","    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n","    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n","    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.8.0+cu126\n","    Uninstalling torch-2.8.0+cu126:\n","      Successfully uninstalled torch-2.8.0+cu126\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.6.0 which is incompatible.\n","torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed colorlog-6.9.0 coreforecast-0.0.16 lightning-utilities-0.15.2 neuralforecast-3.0.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 optuna-4.5.0 pytorch-lightning-2.5.5 ray-2.49.2 sympy-1.13.1 tensorboardX-2.6.4 torch-2.6.0 torchmetrics-1.8.2 triton-3.2.0 utilsforecast-0.2.12\n","Collecting codecarbon\n","  Downloading codecarbon-3.0.5-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: arrow in /usr/local/lib/python3.12/dist-packages (from codecarbon) (1.3.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from codecarbon) (8.2.1)\n","Collecting fief-client[cli] (from codecarbon)\n","  Downloading fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from codecarbon) (2.2.2)\n","Requirement already satisfied: prometheus_client in /usr/local/lib/python3.12/dist-packages (from codecarbon) (0.22.1)\n","Collecting psutil\u003e=6.0.0 (from codecarbon)\n","  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from codecarbon) (9.0.0)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from codecarbon) (2.11.7)\n","Requirement already satisfied: pynvml in /usr/local/lib/python3.12/dist-packages (from codecarbon) (12.0.0)\n","Collecting rapidfuzz (from codecarbon)\n","  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from codecarbon) (2.32.4)\n","Collecting questionary (from codecarbon)\n","  Downloading questionary-2.1.1-py3-none-any.whl.metadata (5.4 kB)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from codecarbon) (13.9.4)\n","Requirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from codecarbon) (0.17.4)\n","Requirement already satisfied: python-dateutil\u003e=2.7.0 in /usr/local/lib/python3.12/dist-packages (from arrow-\u003ecodecarbon) (2.9.0.post0)\n","Requirement already satisfied: types-python-dateutil\u003e=2.8.10 in /usr/local/lib/python3.12/dist-packages (from arrow-\u003ecodecarbon) (2.9.0.20250822)\n","Collecting httpx\u003c0.28.0,\u003e=0.21.3 (from fief-client[cli]-\u003ecodecarbon)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting jwcrypto\u003c2.0.0,\u003e=1.4 (from fief-client[cli]-\u003ecodecarbon)\n","  Downloading jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n","Collecting yaspin (from fief-client[cli]-\u003ecodecarbon)\n","  Downloading yaspin-3.2.0-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: numpy\u003e=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas-\u003ecodecarbon) (2.0.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas-\u003ecodecarbon) (2025.2)\n","Requirement already satisfied: tzdata\u003e=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas-\u003ecodecarbon) (2025.2)\n","Requirement already satisfied: annotated-types\u003e=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-\u003ecodecarbon) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic-\u003ecodecarbon) (2.33.2)\n","Requirement already satisfied: typing-extensions\u003e=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic-\u003ecodecarbon) (4.15.0)\n","Requirement already satisfied: typing-inspection\u003e=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-\u003ecodecarbon) (0.4.1)\n","Requirement already satisfied: nvidia-ml-py\u003c13.0.0a0,\u003e=12.0.0 in /usr/local/lib/python3.12/dist-packages (from pynvml-\u003ecodecarbon) (12.575.51)\n","Requirement already satisfied: prompt_toolkit\u003c4.0,\u003e=2.0 in /usr/local/lib/python3.12/dist-packages (from questionary-\u003ecodecarbon) (3.0.52)\n","Requirement already satisfied: charset_normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ecodecarbon) (3.4.3)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ecodecarbon) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ecodecarbon) (2.5.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ecodecarbon) (2025.8.3)\n","Requirement already satisfied: markdown-it-py\u003e=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich-\u003ecodecarbon) (4.0.0)\n","Requirement already satisfied: pygments\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich-\u003ecodecarbon) (2.19.2)\n","Requirement already satisfied: shellingham\u003e=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer-\u003ecodecarbon) (1.5.4)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx\u003c0.28.0,\u003e=0.21.3-\u003efief-client[cli]-\u003ecodecarbon) (4.10.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx\u003c0.28.0,\u003e=0.21.3-\u003efief-client[cli]-\u003ecodecarbon) (1.0.9)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from httpx\u003c0.28.0,\u003e=0.21.3-\u003efief-client[cli]-\u003ecodecarbon) (1.3.1)\n","Requirement already satisfied: h11\u003e=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*-\u003ehttpx\u003c0.28.0,\u003e=0.21.3-\u003efief-client[cli]-\u003ecodecarbon) (0.16.0)\n","Requirement already satisfied: cryptography\u003e=3.4 in /usr/local/lib/python3.12/dist-packages (from jwcrypto\u003c2.0.0,\u003e=1.4-\u003efief-client[cli]-\u003ecodecarbon) (43.0.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py\u003e=2.2.0-\u003erich-\u003ecodecarbon) (0.1.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit\u003c4.0,\u003e=2.0-\u003equestionary-\u003ecodecarbon) (0.2.13)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil\u003e=2.7.0-\u003earrow-\u003ecodecarbon) (1.17.0)\n","Requirement already satisfied: termcolor\u003c4.0,\u003e=3.1 in /usr/local/lib/python3.12/dist-packages (from yaspin-\u003efief-client[cli]-\u003ecodecarbon) (3.1.0)\n","Requirement already satisfied: cffi\u003e=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography\u003e=3.4-\u003ejwcrypto\u003c2.0.0,\u003e=1.4-\u003efief-client[cli]-\u003ecodecarbon) (2.0.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi\u003e=1.12-\u003ecryptography\u003e=3.4-\u003ejwcrypto\u003c2.0.0,\u003e=1.4-\u003efief-client[cli]-\u003ecodecarbon) (2.23)\n","Downloading codecarbon-3.0.5-py3-none-any.whl (278 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.2/291.2 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading questionary-2.1.1-py3-none-any.whl (36 kB)\n","Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fief_client-0.20.0-py3-none-any.whl (20 kB)\n","Downloading yaspin-3.2.0-py3-none-any.whl (20 kB)\n","Installing collected packages: yaspin, rapidfuzz, psutil, questionary, httpx, jwcrypto, fief-client, codecarbon\n","  Attempting uninstall: psutil\n","    Found existing installation: psutil 5.9.5\n","    Uninstalling psutil-5.9.5:\n","      Successfully uninstalled psutil-5.9.5\n","  Attempting uninstall: httpx\n","    Found existing installation: httpx 0.28.1\n","    Uninstalling httpx-0.28.1:\n","      Successfully uninstalled httpx-0.28.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-genai 1.34.0 requires httpx\u003c1.0.0,\u003e=0.28.1, but you have httpx 0.27.2 which is incompatible.\n","firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed codecarbon-3.0.5 fief-client-0.20.0 httpx-0.27.2 jwcrypto-1.5.6 psutil-7.1.0 questionary-2.1.1 rapidfuzz-3.14.1 yaspin-3.2.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"c959eee482f147099236f406c067c1bc","pip_warning":{"packages":["psutil"]}}},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Found existing installation: torch 2.6.0\n","Uninstalling torch-2.6.0:\n","  Successfully uninstalled torch-2.6.0\n","Found existing installation: torchvision 0.23.0+cu126\n","Uninstalling torchvision-0.23.0+cu126:\n","  Successfully uninstalled torchvision-0.23.0+cu126\n","Found existing installation: torchaudio 2.8.0+cu126\n","Uninstalling torchaudio-2.8.0+cu126:\n","  Successfully uninstalled torchaudio-2.8.0+cu126\n","Found existing installation: neuralforecast 3.0.2\n","Uninstalling neuralforecast-3.0.2:\n","  Successfully uninstalled neuralforecast-3.0.2\n","Found existing installation: pytorch-lightning 2.5.5\n","Uninstalling pytorch-lightning-2.5.5:\n","  Successfully uninstalled pytorch-lightning-2.5.5\n","Found existing installation: lightning-utilities 0.15.2\n","Uninstalling lightning-utilities-0.15.2:\n","  Successfully uninstalled lightning-utilities-0.15.2\n","Collecting neuralforecast[models]\n","  Using cached neuralforecast-3.0.2-py3-none-any.whl.metadata (14 kB)\n","\u001b[33mWARNING: neuralforecast 3.0.2 does not provide the extra 'models'\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: coreforecast\u003e=0.0.6 in /usr/local/lib/python3.12/dist-packages (from neuralforecast[models]) (0.0.16)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from neuralforecast[models]) (2025.3.0)\n","Requirement already satisfied: numpy\u003e=1.21.6 in /usr/local/lib/python3.12/dist-packages (from neuralforecast[models]) (2.0.2)\n","Requirement already satisfied: pandas\u003e=1.3.5 in /usr/local/lib/python3.12/dist-packages (from neuralforecast[models]) (2.2.2)\n","Collecting torch\u003c=2.6.0,\u003e=2.0.0 (from neuralforecast[models])\n","  Using cached torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n","Collecting pytorch-lightning\u003e=2.0.0 (from neuralforecast[models])\n","  Using cached pytorch_lightning-2.5.5-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: ray\u003e=2.2.0 in /usr/local/lib/python3.12/dist-packages (from ray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (2.49.2)\n","Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (from neuralforecast[models]) (4.5.0)\n","Requirement already satisfied: utilsforecast\u003e=0.2.3 in /usr/local/lib/python3.12/dist-packages (from neuralforecast[models]) (0.2.12)\n","Requirement already satisfied: python-dateutil\u003e=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas\u003e=1.3.5-\u003eneuralforecast[models]) (2.9.0.post0)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas\u003e=1.3.5-\u003eneuralforecast[models]) (2025.2)\n","Requirement already satisfied: tzdata\u003e=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas\u003e=1.3.5-\u003eneuralforecast[models]) (2025.2)\n","Requirement already satisfied: tqdm\u003e=4.57.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning\u003e=2.0.0-\u003eneuralforecast[models]) (4.67.1)\n","Requirement already satisfied: PyYAML\u003e5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning\u003e=2.0.0-\u003eneuralforecast[models]) (6.0.2)\n","Requirement already satisfied: torchmetrics\u003e0.7.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning\u003e=2.0.0-\u003eneuralforecast[models]) (1.8.2)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning\u003e=2.0.0-\u003eneuralforecast[models]) (25.0)\n","Requirement already satisfied: typing-extensions\u003e4.5.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning\u003e=2.0.0-\u003eneuralforecast[models]) (4.15.0)\n","Collecting lightning-utilities\u003e=0.10.0 (from pytorch-lightning\u003e=2.0.0-\u003eneuralforecast[models])\n","  Using cached lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: click\u003e=7.0 in /usr/local/lib/python3.12/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (8.2.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (3.19.1)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (4.25.1)\n","Requirement already satisfied: msgpack\u003c2.0.0,\u003e=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (1.1.1)\n","Requirement already satisfied: protobuf\u003e=3.20.3 in /usr/local/lib/python3.12/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (5.29.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (2.32.4)\n","Requirement already satisfied: tensorboardX\u003e=1.9 in /usr/local/lib/python3.12/dist-packages (from ray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (2.6.4)\n","Requirement already satisfied: pyarrow\u003e=9.0.0 in /usr/local/lib/python3.12/dist-packages (from ray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (18.1.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (3.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (75.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (1.13.1)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1-\u003etorch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (1.3.0)\n","Requirement already satisfied: alembic\u003e=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna-\u003eneuralforecast[models]) (1.16.5)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna-\u003eneuralforecast[models]) (6.9.0)\n","Requirement already satisfied: sqlalchemy\u003e=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna-\u003eneuralforecast[models]) (2.0.43)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic\u003e=1.5.0-\u003eoptuna-\u003eneuralforecast[models]) (1.3.10)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast[models]) (3.12.15)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil\u003e=2.8.2-\u003epandas\u003e=1.3.5-\u003eneuralforecast[models]) (1.17.0)\n","Requirement already satisfied: greenlet\u003e=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy\u003e=1.4.2-\u003eoptuna-\u003eneuralforecast[models]) (3.2.4)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2-\u003etorch\u003c=2.6.0,\u003e=2.0.0-\u003eneuralforecast[models]) (3.0.2)\n","Requirement already satisfied: attrs\u003e=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications\u003e=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (2025.9.1)\n","Requirement already satisfied: referencing\u003e=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (0.36.2)\n","Requirement already satisfied: rpds-py\u003e=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (0.27.1)\n","Requirement already satisfied: charset_normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.12/dist-packages (from requests-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (3.4.3)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (2.5.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast[models]) (2025.8.3)\n","Requirement already satisfied: aiohappyeyeballs\u003e=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast[models]) (2.6.1)\n","Requirement already satisfied: aiosignal\u003e=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast[models]) (1.4.0)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast[models]) (1.7.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast[models]) (6.6.4)\n","Requirement already satisfied: propcache\u003e=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast[models]) (0.3.2)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast[models]) (1.20.1)\n","Using cached pytorch_lightning-2.5.5-py3-none-any.whl (832 kB)\n","Using cached torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n","Using cached neuralforecast-3.0.2-py3-none-any.whl (260 kB)\n","Using cached lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n","Installing collected packages: lightning-utilities, torch, pytorch-lightning, neuralforecast\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","timm 1.0.19 requires torchvision, which is not installed.\n","fastai 2.8.4 requires torchvision\u003e=0.11, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed lightning-utilities-0.15.2 neuralforecast-3.0.2 pytorch-lightning-2.5.5 torch-2.6.0\n"]}],"source":["!pip install neuralforecast\n","# !pip install torchinfo\n","!pip install codecarbon\n","\n","# ==============================================================================\n","# SEL INSTALASI - JALANKAN INI DULU\n","# ==============================================================================\n","# Uninstall versi yang mungkin konflik untuk memastikan instalasi bersih\n","!pip uninstall -y torch torchvision torchaudio neuralforecast pytorch-lightning lightning-utilities\n","\n","# Install neuralforecast dengan dependensi modelnya.\n","# Pip akan secara otomatis memilih versi torch, torchvision, dll. yang kompatibel.\n","!pip install \"neuralforecast[models]\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"0-XLBNaFuwJY"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mencoba mount Google Drive...\n","Gagal mount Google Drive atau membuat folder: mount failed\n","Hasil TIDAK akan disimpan ke Google Drive.\n","\n","-- Memuat Dataset 1: Australian Electricity Demand --\n","Mengunduh data dari: https://raw.githubusercontent.com/kanadakurniawan/loss-function-comparison/5b204ef45db85a9ff4e283dd74941dbc117ad287/dataset/australian_electricity_demand_dataset.tsf\n","Data berhasil diunduh.\n","Konten decode dgn 'utf-8'.\n","Memulai parsing TSF (varian: australia)...\n"]},{"name":"stderr","output_type":"stream","text":["[codecarbon WARNING @ 04:44:23] Multiple instances of codecarbon are allowed to run at the same time.\n"]},{"name":"stdout","output_type":"stream","text":["Parsing TSF selesai. 5 series berhasil diparsing.\n","\n","Memilih series: 'T1_NSW' (Index: 0) dari 'Australian Electricity Demand'\n","  -\u003e 230736 titik data, mulai dari 2002-01-01 00:00:00+00:00.\n","Tidak ada nilai NaN.\n","Standar Deviasi Data Asli (untuk normalisasi residual): 1361.9154\n","Faktor Skala MASE (In-sample Naive MAE): 174.8714\n","\n","Normalisasi data (MinMaxScaler [0, 1])...\n","\n","Membuat fold CV dgn parameter tetap...\n","\n","Membuat 3 fold CV:\n","  Freq=30min, DataLen=230736, InputWin=168, Horizon=24, Step=24, TrainWin=230616\n","  Fold 1: Train[0:230616](len=230616), Test[230616:230640](len=24)\n","  Fold 2: Train[24:230640](len=230616), Test[230640:230664](len=24)\n","  Fold 3: Train[48:230664](len=230616), Test[230664:230688](len=24)\n","\n","--- Konfigurasi Training Adaptif Dihitung ---\n","  Panjang Train per Fold: 230616\n","  Batch per Epoch       : 7207\n","  Target Epochs         : 50 -\u003e (Target Steps: 360350)\n","  Min/Max Steps Budget  : 500 / 40000\n","  FINAL Max Steps/Fold  : 40000\n","  FINAL Freq. Validasi  : Setiap 2000 langkah\n","  FINAL Patience (steps): 8000 langkah\n","\n","===== Memulai Eksperimen untuk Model: PatchTST =====\n","\n","===== Menjalankan Eksperimen untuk Loss: MSE =====\n","\n","Inisialisasi model PatchTST dgn loss MSE...\n","Model PatchTST \u0026 NeuralForecast diinisialisasi untuk loss MSE.\n","\n","Memulai Cross-Validation untuk loss MSE...\n","\n","--- Processing Fold 1/3 (Loss: MSE) ---\n"]},{"name":"stderr","output_type":"stream","text":["[codecarbon WARNING @ 04:44:25] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n","[codecarbon WARNING @ 04:44:25] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n"," Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n","\n","[codecarbon WARNING @ 04:44:25] No CPU tracking mode found. Falling back on CPU constant mode.\n"]},{"name":"stdout","output_type":"stream","text":["Training model fold 1 (Loss: MSE, Train: 230616, Val: 24, MaxSteps: 40000)...\n"]},{"name":"stderr","output_type":"stream","text":["[codecarbon WARNING @ 04:44:41] Background scheduler didn't run for a long period (4s), results might be inaccurate\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09799285e5cf401990bfa1d0e4a460d1","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc561482acfb45a7a6be940a21154ff0","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# ==============================================================================\n","# 1. Import Libraries\n","# ==============================================================================\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from scipy.stats import skew, kurtosis\n","from neuralforecast import NeuralForecast\n","# 1.1 Import Semua Model yang Akan Digunakan\n","from neuralforecast.models import NBEATS, NHITS, LSTM, TCN, NLinear, PatchTST, TiDE\n","# 1.2 Import Semua Loss Function yang Akan Digunakan\n","from neuralforecast.losses.pytorch import MAE, MSE, HuberLoss, MAPE, SMAPE, QuantileLoss\n","from sklearn.preprocessing import MinMaxScaler\n","import time\n","import psutil\n","import os\n","from codecarbon import EmissionsTracker\n","import random\n","import torch\n","import logging\n","import requests\n","from io import StringIO\n","import traceback\n","try:\n","    from google.colab import drive\n","    GOOGLE_COLAB = True\n","except ImportError:\n","    GOOGLE_COLAB = False\n","\n","# ==============================================================================\n","# 2. Konfigurasi \u0026 Hyperparameters\n","# ==============================================================================\n","# 2.1 Pengaturan Reproducibility\n","seed = 42\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.manual_seed(seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(seed)\n","\n","# 2.2 Pengaturan Eksperimen Utama\n","SELECTED_DATASET_INDEX = 1\n","SELECTED_SERIES_INDEX = 0\n","\n","# 2.2.1 Pemilihan Model\n","# Pilih INDEKS model yang akan digunakan dari MODEL_CONFIG di bawah\n","# 1: LSTM, 2: TCN, 3: N-BEATS, 4: NHITS, 5: N-Linear, 6: PatchTST, 7: TiDE\n","SELECTED_MODEL_INDEX = 6\n","\n","# 2.3 Konfigurasi Detail Dataset\n","DATASET_CONFIG = {\n","    1: {'name': 'Australian Electricity Demand','type': 'tsf','source': \"https://raw.githubusercontent.com/kanadakurniawan/loss-function-comparison/5b204ef45db85a9ff4e283dd74941dbc117ad287/dataset/australian_electricity_demand_dataset.tsf\",'freq': '30min','parser_variant': 'australia','value_column': None, 'time_column': None,},\n","    2: {'name': 'Bike Sharing Daily','type': 'csv','source': 'https://raw.githubusercontent.com/kanadakurniawan/loss-function-comparison/e2e72b49171794fbc1f285f3eaceff32a8faa5e8/dataset/bike_sharing_dataset.csv','freq': 'D','parser_variant': None,'value_column': 'cnt', 'time_column': 'dteday',},\n","    3: {'name': 'M4 Daily','type': 'tsf','source': '/content/drive/My Drive/S2/Thesis/loss-function-comparison/dataset/m4_daily_dataset.tsf','freq': 'D','parser_variant': 'standard','value_column': None, 'time_column': None,},\n","    4: {'name': 'M4 Hourly','type': 'tsf','source': 'https://raw.githubusercontent.com/kanadakurniawan/loss-function-comparison/e2e72b49171794fbc1f285f3eaceff32a8faa5e8/dataset/m4_hourly_dataset.tsf','freq': 'H','parser_variant': 'standard','value_column': None, 'time_column': None,},\n","    5: {'name': 'Tourism Monthly','type': 'tsf','source': 'https://raw.githubusercontent.com/kanadakurniawan/loss-function-comparison/e2e72b49171794fbc1f285f3eaceff32a8faa5e8/dataset/tourism_monthly_dataset.tsf','freq': 'MS','parser_variant': 'standard','value_column': None, 'time_column': None,},\n","    6: {'name': 'Solar Power Generation','type': 'csv','source': '/content/drive/My Drive/S2/Thesis/loss-function-comparison/dataset/solar_power_generation_dataset_1.csv','freq': '15min','parser_variant': None,'value_column': 'DC_POWER', 'time_column': 'DATE_TIME',}\n","}\n","\n","# 2.4 Konfigurasi Detail Model\n","MODEL_CONFIG = {\n","    1: {'name': 'LSTM', 'class': LSTM},\n","    2: {'name': 'TCN', 'class': TCN},\n","    3: {'name': 'NBEATS', 'class': NBEATS},\n","    4: {'name': 'NHITS', 'class': NHITS},\n","    5: {'name': 'NLinear', 'class': NLinear},\n","    6: {'name': 'PatchTST', 'class': PatchTST},\n","    7: {'name': 'TiDE', 'class': TiDE}\n","}\n","\n","# 2.5 Pengaturan Preprocessing\n","NAN_IMPUTATION_METHOD = 'ffill_bfill'\n","\n","# 2.6 Pengaturan Model \u0026 Training\n","INPUT_WINDOW_SIZE = 168\n","HORIZON = 24\n","BATCH_SIZE = 32\n","VALIDATION_SIZE_FIT = HORIZON\n","\n","# 2.6.1 Konfigurasi Training Adaptif\n","TARGET_EPOCHS_PER_FOLD = 50\n","MIN_TRAINING_STEPS = 500\n","MAX_TRAINING_BUDGET_STEPS = 40000\n","EARLY_STOP_PATIENCE_VALIDATIONS = 4\n","\n","# 2.7 Pengaturan Cross-Validation\n","N_CROSSVALIDATION_FOLDS = 3\n","CV_STEP_SIZE = 24\n","\n","# 2.8 Pengaturan Lainnya\n","os.environ['NIXTLA_ID_AS_COL'] = '1'\n","logging.getLogger(\"codecarbon\").setLevel(logging.WARNING)\n","logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n","logging.getLogger(\"lightning_fabric\").setLevel(logging.WARNING)\n","GDRIVE_OUTPUT_FOLDER = \"/content/drive/My Drive/S2/Thesis/loss-function-comparison/hasil/\"\n","\n","# ==============================================================================\n","# 3. Inisialisasi Lingkungan (Google Drive)\n","# ==============================================================================\n","if GOOGLE_COLAB:\n","    print(\"Mencoba mount Google Drive...\")\n","    try:\n","        drive.mount('/content/drive', force_remount=True)\n","        print(f\"Google Drive di-mount. Memeriksa/membuat folder output: {GDRIVE_OUTPUT_FOLDER}\")\n","        os.makedirs(GDRIVE_OUTPUT_FOLDER, exist_ok=True)\n","        print(f\"Folder output '{GDRIVE_OUTPUT_FOLDER}' siap.\")\n","    except Exception as e:\n","        print(f\"Gagal mount Google Drive atau membuat folder: {e}\")\n","        print(\"Hasil TIDAK akan disimpan ke Google Drive.\")\n","        GDRIVE_OUTPUT_FOLDER = None\n","else:\n","    print(\"Bukan lingkungan Google Colab, hasil tidak akan disimpan ke Google Drive.\")\n","    GDRIVE_OUTPUT_FOLDER = None\n","\n","# ==============================================================================\n","# 4. Fungsi Helper\n","# ==============================================================================\n","# (Semua fungsi helper dari jawaban sebelumnya ada di sini)\n","def load_data_from_source(source_path_or_url):\n","    \"\"\"Membaca konten data dari URL atau file lokal.\"\"\"\n","    source_str = str(source_path_or_url)\n","    if source_str.startswith('http'):\n","        print(f\"Mengunduh data dari: {source_str}\")\n","        try:\n","            response = requests.get(source_str, timeout=60)\n","            response.raise_for_status(); print(\"Data berhasil diunduh.\")\n","            content_bytes = response.content; decoded_content = None; header_encoding = response.encoding\n","            encodings_to_try = ['utf-8', 'iso-8859-1', 'latin1', 'cp1252']\n","            if header_encoding and header_encoding.lower() not in [e.lower() for e in encodings_to_try]: encodings_to_try.insert(0, header_encoding)\n","            elif header_encoding:\n","                 try: idx = [e.lower() for e in encodings_to_try].index(header_encoding.lower()); encodings_to_try.insert(0, encodings_to_try.pop(idx))\n","                 except ValueError: encodings_to_try.insert(0, header_encoding)\n","            for enc in encodings_to_try:\n","                try: decoded_content = content_bytes.decode(enc); print(f\"Konten decode dgn '{enc}'.\"); break\n","                except UnicodeDecodeError: continue\n","                except Exception as decode_err: print(f\" Error decode dgn '{enc}': {decode_err}\"); continue\n","            if decoded_content is None:\n","                print(f\"Error: Gagal decode konten URL.\")\n","                try: decoded_content = content_bytes.decode('utf-8', errors='replace'); print(\"Warn: Decode final dgn utf-8 replace.\")\n","                except Exception: print(\"Gagal total decoding.\"); return None\n","            return decoded_content\n","        except requests.exceptions.RequestException as e: print(f\"Error unduh data: {e}\"); return None\n","        except Exception as e: print(f\"Error lain proses URL: {e}\"); print(traceback.format_exc()); return None\n","    else:\n","        print(f\"Membaca data dari file lokal: {source_str}\")\n","        try:\n","            if not os.path.exists(source_str):\n","                print(f\"Error: File tdk ditemukan: {source_str}\")\n","                if 'drive/My Drive' in source_str: print(\"Tips: Pastikan Drive ter-mount.\")\n","                return None\n","            encodings_to_try = ['utf-8', 'iso-8859-1', 'latin1', 'cp1252']\n","            content = None\n","            for enc in encodings_to_try:\n","                 try:\n","                     with open(source_str, 'r', encoding=enc) as f: content = f.read()\n","                     print(f\"Data dibaca (lokal) dgn encoding '{enc}'.\")\n","                     break\n","                 except UnicodeDecodeError: pass\n","                 except Exception as e: print(f\"Error baca file {source_str} dgn enc '{enc}': {e}\"); return None\n","            if content is None: print(\"Error: Gagal baca file lokal dgn encoding yg dicoba.\"); return None\n","            return content\n","        except Exception as e: print(f\"Error tak terduga baca file lokal {source_str}: {e}\"); return None\n","\n","def parse_tsf_data(raw_content, parser_variant):\n","    \"\"\"Mem-parsing data TSF mentah (dari string).\"\"\"\n","    parsed_series, series_ids, start_times = [], [], []\n","    print(f\"Memulai parsing TSF (varian: {parser_variant})...\")\n","    lines = raw_content.splitlines(); reading_data = False; skipped_lines = 0; parsed_count = 0\n","    for i, line in enumerate(lines):\n","        line = line.strip()\n","        if not line or line.startswith((\"#\", \"@relation\", \"@attribute\", \"@frequency\", \"@horizon\", \"@missing\", \"@equallength\")): continue\n","        if line.startswith(\"@data\"): reading_data = True; continue\n","        if reading_data:\n","            parts = line.split(\":\")\n","            try:\n","                if parser_variant == 'australia' and len(parts) \u003e= 4:\n","                    series_name, state_name, start_time_str, values_str = parts[0], parts[1], parts[2], parts[3]; unique_id = f\"{series_name}_{state_name}\"\n","                elif parser_variant == 'standard' and len(parts) \u003e= 3:\n","                    series_name, start_time_str, values_str = parts[0], parts[1], parts[2]; unique_id = series_name\n","                else: skipped_lines += 1; continue\n","                try: start_time = pd.Timestamp(start_time_str.replace(' ', 'T'))\n","                except ValueError:\n","                     try: start_time = pd.to_datetime(start_time_str, format='%Y-%m-%d %H-%M-%S')\n","                     except ValueError: skipped_lines += 1; continue\n","                time_series = []\n","                for val_str in values_str.split(\",\"):\n","                    val_str = val_str.strip()\n","                    if val_str and val_str != '?':\n","                        try: time_series.append(float(val_str))\n","                        except ValueError: time_series.append(np.nan)\n","                    elif val_str == '?': time_series.append(np.nan)\n","                if time_series: parsed_series.append(time_series); series_ids.append(unique_id); start_times.append(start_time); parsed_count += 1\n","            except Exception as e: skipped_lines += 1; pass\n","    if skipped_lines \u003e 0: print(f\"Peringatan: Melewati {skipped_lines} baris saat parsing TSF.\")\n","    print(f\"Parsing TSF selesai. {parsed_count} series berhasil diparsing.\")\n","    return series_ids, start_times, parsed_series\n","\n","def parse_csv_data(raw_content, time_col, value_col, dataset_name):\n","    \"\"\"Mem-parsing data CSV mentah (dari string).\"\"\"\n","    print(f\"Memulai parsing CSV (time: '{time_col}', value: '{value_col}')...\")\n","    try:\n","        df = pd.read_csv(StringIO(raw_content))\n","        if time_col not in df.columns: raise ValueError(f\"Kolom waktu '{time_col}' tidak ada. Kolom: {df.columns.tolist()}\")\n","        if value_col not in df.columns: raise ValueError(f\"Kolom nilai '{value_col}' tidak ada. Kolom: {df.columns.tolist()}\")\n","        try: df[time_col] = pd.to_datetime(df[time_col])\n","        except (ValueError, TypeError):\n","            print(f\"Gagal parse '{time_col}' dgn format default, coba 'dd-mm-yyyy HH:MM'...\")\n","            try: df[time_col] = pd.to_datetime(df[time_col], format='%d-%m-%Y %H:%M')\n","            except Exception as e_fmt: raise ValueError(f\"Gagal parse '{time_col}' dgn format yg dicoba: {e_fmt}\")\n","        df[value_col] = pd.to_numeric(df[value_col], errors='coerce')\n","        df = df.sort_values(by=time_col).reset_index(drop=True)\n","        start_time = df[time_col].iloc[0]; time_series = df[value_col].tolist()\n","        unique_id = f\"{dataset_name.replace(' ', '_')}_Series\"\n","        print(f\"Parsing CSV selesai. 1 series, {len(time_series)} titik data.\")\n","        return [unique_id], [start_time], [time_series]\n","    except Exception as e: print(f\"Error parsing CSV: {e}\"); print(traceback.format_exc()); return [], [], []\n","\n","def load_and_parse_data(dataset_index, config):\n","    \"\"\"Fungsi utama untuk memuat dan mem-parsing dataset berdasarkan Indeks.\"\"\"\n","    cfg = config.get(dataset_index);\n","    if not cfg: raise ValueError(f\"Indeks Dataset '{dataset_index}' tidak ditemukan.\")\n","    dataset_name = cfg['name']\n","    print(f\"\\n-- Memuat Dataset {dataset_index}: {dataset_name} --\")\n","    raw_content = load_data_from_source(cfg['source'])\n","    if raw_content is None: print(f\"Gagal memuat konten untuk {dataset_name}.\"); return None, None, None, None, None\n","    data_freq = cfg['freq']\n","    if cfg['type'] == 'tsf': ids, starts, series_list = parse_tsf_data(raw_content, cfg['parser_variant'])\n","    elif cfg['type'] == 'csv': ids, starts, series_list = parse_csv_data(raw_content, cfg['time_column'], cfg['value_column'], dataset_name)\n","    else: raise ValueError(f\"Tipe dataset '{cfg['type']}' tidak dikenal.\")\n","    if not ids: print(f\"Tidak ada series diparsing untuk {dataset_name}.\"); return None, None, None, None, None\n","    return ids, starts, series_list, data_freq, dataset_name\n","\n","def select_series(all_ids, all_start_times, all_series_data, index, dataset_name):\n","    \"\"\"Memilih time series spesifik dari hasil parsing berdasarkan indeks.\"\"\"\n","    if not all_ids: raise ValueError(f\"Tidak ada data series tersedia u/ dipilih dari '{dataset_name}'.\")\n","    if not (0 \u003c= index \u003c len(all_ids)): raise ValueError(f\"Indeks series {index} tdk valid u/ '{dataset_name}'. Pilih 0-{len(all_ids) - 1}.\")\n","    selected_id = all_ids[index]; start_time = all_start_times[index]; time_series = all_series_data[index]\n","    print(f\"\\nMemilih series: '{selected_id}' (Index: {index}) dari '{dataset_name}'\")\n","    if not time_series: print(f\"Peringatan: Time series '{selected_id}' kosong.\"); return selected_id, start_time, []\n","    print(f\"  -\u003e {len(time_series)} titik data, mulai dari {start_time}.\")\n","    return selected_id, start_time, time_series\n","\n","def handle_nan_values(ts, method='ffill_bfill'):\n","    \"\"\"Menangani nilai NaN dalam time series (input berupa list).\"\"\"\n","    if not isinstance(ts, (list, np.ndarray, pd.Series)): raise TypeError(\"Input 'ts' hrs list/array/Series.\")\n","    if len(ts) == 0: print(\"Warn: TS kosong sblm handle NaN.\"); return []\n","    ts_series = pd.Series(ts, dtype=float); initial_nan_count = ts_series.isna().sum()\n","    if initial_nan_count == 0: print(\"Tidak ada nilai NaN.\"); return ts_series.tolist()\n","    print(f\"Menangani {initial_nan_count}/{len(ts_series)} NaN dgn metode: {method}\")\n","    if method == 'ffill_bfill': filled_ts = ts_series.ffill().bfill()\n","    elif method == 'mean': mean_val = ts_series.mean(); filled_ts = ts_series.fillna(mean_val if pd.notna(mean_val) else 0); print(f\"  Imputasi mean: {mean_val:.4f}\" if pd.notna(mean_val) else \" (rata2 NaN)\")\n","    elif method == 'median': median_val = ts_series.median(); filled_ts = ts_series.fillna(median_val if pd.notna(median_val) else 0); print(f\"  Imputasi median: {median_val:.4f}\" if pd.notna(median_val) else \" (median NaN)\")\n","    elif method == 'interpolate_linear': filled_ts = ts_series.interpolate(method='linear', limit_direction='both').ffill().bfill()\n","    else: print(f\"Warn: Metode '{method}' tdk dikenal. Pakai 'ffill_bfill'.\"); filled_ts = ts_series.ffill().bfill()\n","    final_nan_count = filled_ts.isna().sum()\n","    if final_nan_count \u003e 0: print(f\"Warn: Masih ada {final_nan_count} NaN! Isi dgn 0.\"); filled_ts = filled_ts.fillna(0)\n","    else: print(\"Semua NaN berhasil ditangani.\")\n","    return filled_ts.tolist()\n","\n","def prepare_dataframe_for_neuralforecast(time_series, unique_id, start_time, freq):\n","    \"\"\"Mempersiapkan Pandas DataFrame dalam format yang dibutuhkan NeuralForecast.\"\"\"\n","    if not isinstance(time_series, (list, np.ndarray)): raise TypeError(\"time_series hrs list/array\")\n","    if len(time_series) == 0: raise ValueError(\"time_series kosong\")\n","    if not isinstance(start_time, pd.Timestamp):\n","         try: start_time = pd.Timestamp(start_time)\n","         except Exception as e: raise ValueError(f\"start_time tdk valid: {start_time} - {e}\")\n","    if not freq: raise ValueError(\"freq tdk boleh kosong/None\")\n","    try: timestamps = pd.date_range(start=start_time, periods=len(time_series), freq=freq)\n","    except ValueError as e: raise ValueError(f\"Gagal buat date_range: start={start_time}, periods={len(time_series)}, freq='{freq}': {e}\")\n","    df = pd.DataFrame({\"ds\": timestamps, \"y\": time_series}); df[\"unique_id\"] = unique_id\n","    df['y'] = df['y'].astype(float); return df\n","\n","def create_timeseries_cv_folds(data, horizon, step_size, n_crossvalidation, freq, input_window_size):\n","    \"\"\"Membagi data time series menjadi beberapa fold untuk cross-validation (sliding window).\"\"\"\n","    if not isinstance(data, np.ndarray):\n","        try: data = np.array(data, dtype=float)\n","        except ValueError: raise TypeError(\"Input 'data' CV hrs array float.\")\n","    if np.isnan(data).any(): print(\"Warn: NaN di data input create_timeseries_cv_folds.\")\n","    dataset_length = len(data);\n","    if dataset_length == 0: raise ValueError(\"Data input CV kosong.\")\n","    if not all(isinstance(x, int) and x \u003e 0 for x in [horizon, step_size, n_crossvalidation, input_window_size]): raise ValueError(\"Parameter CV hrs int positif.\")\n","    total_test_step_length = horizon * n_crossvalidation + step_size * (n_crossvalidation - 1)\n","    min_length_needed = total_test_step_length + input_window_size\n","    if dataset_length \u003c min_length_needed: raise ValueError(f\"Dataset pendek ({dataset_length}) u/ CV. Butuh min {min_length_needed} (H={horizon},F={n_crossvalidation},S={step_size},I={input_window_size}).\")\n","    train_window_length = dataset_length - total_test_step_length\n","    if train_window_length \u003c input_window_size: raise ValueError(f\"Train window ({train_window_length}) \u003c input size ({input_window_size}).\")\n","    print(f\"\\nMembuat {n_crossvalidation} fold CV:\")\n","    print(f\"  Freq={freq}, DataLen={dataset_length}, InputWin={input_window_size}, Horizon={horizon}, Step={step_size}, TrainWin={train_window_length}\")\n","    folds = []\n","    for i in range(n_crossvalidation):\n","        start_train = i * step_size; end_train = start_train + train_window_length\n","        start_test = end_train; end_test = start_test + horizon\n","        if end_test \u003e dataset_length: print(f\"Warn: Fold {i+1} melebihi data. Stop.\"); break\n","        train_fold_data = data[start_train:end_train]; test_fold_data = data[start_test:end_test]\n","        if len(train_fold_data) == 0 or len(test_fold_data) == 0: print(f\"Warn: Fold {i+1} kosong. Stop.\"); break\n","        folds.append((train_fold_data, test_fold_data))\n","        print(f\"  Fold {i+1}: Train[{start_train}:{end_train}](len={len(train_fold_data)}), Test[{start_test}:{end_test}](len={len(test_fold_data)})\")\n","    actual_folds_created = len(folds)\n","    if actual_folds_created == 0: raise RuntimeError(\"Gagal buat fold CV.\")\n","    elif actual_folds_created \u003c n_crossvalidation: print(f\"\\nPeringatan: Hanya {actual_folds_created}/{n_crossvalidation} fold dibuat.\")\n","    return folds\n","\n","def denormalize(data_normalized, scaler):\n","    \"\"\"Mengembalikan data yang dinormalisasi ke skala aslinya.\"\"\"\n","    if isinstance(data_normalized, (int, float)): data_normalized = np.array([data_normalized])\n","    elif isinstance(data_normalized, pd.Series): data_normalized = data_normalized.to_numpy()\n","    elif isinstance(data_normalized, list): data_normalized = np.array(data_normalized)\n","    if not isinstance(data_normalized, np.ndarray): raise TypeError(\"Input denorm hrs array.\")\n","    is_scaler_valid = hasattr(scaler, 'scale_') and scaler.scale_ is not None and not np.all(scaler.scale_ == 0)\n","    if not is_scaler_valid:\n","        if hasattr(scaler, 'min_') and scaler.min_ is not None: return np.full(data_normalized.shape, scaler.min_[0])\n","        else: return data_normalized.flatten()\n","    try:\n","        if data_normalized.ndim == 1: data_reshaped = data_normalized.reshape(-1, 1)\n","        elif data_normalized.ndim == 2 and data_normalized.shape[1] == 1: data_reshaped = data_normalized\n","        else: raise ValueError(f\"Input denorm hrs 1D/2D(1 col). Shape: {data_normalized.shape}\")\n","        if np.isnan(data_reshaped).any(): print(\"Warn: NaN sblm inverse_transform.\")\n","        data_denormalized = scaler.inverse_transform(data_reshaped)\n","        if np.isnan(data_denormalized).any(): print(\"Warn: NaN stlh inverse_transform.\")\n","        return data_denormalized.flatten()\n","    except Exception as e: print(f\"Error denorm: {e}. Input shape: {data_normalized.shape}\"); print(traceback.format_exc()); return data_normalized.flatten()\n","\n","def mean_absolute_percentage_error(y_true, y_pred):\n","    \"\"\"Menghitung MAPE, handle pembagian nol dan NaN.\"\"\"\n","    y_true, y_pred = np.array(y_true), np.array(y_pred)\n","    valid_mask = ~np.isnan(y_true) \u0026 ~np.isnan(y_pred); y_true, y_pred = y_true[valid_mask], y_pred[valid_mask]\n","    if len(y_true) == 0: return np.inf\n","    mask = y_true != 0\n","    if np.sum(mask) == 0: return 0.0 if np.allclose(y_pred, 0) else np.inf\n","    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n","    return mape if np.isfinite(mape) else np.finfo(np.float64).max\n","\n","def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n","    \"\"\"Menghitung sMAPE, handle pembagian nol dan NaN.\"\"\"\n","    y_true, y_pred = np.array(y_true), np.array(y_pred)\n","    valid_mask = ~np.isnan(y_true) \u0026 ~np.isnan(y_pred); y_true, y_pred = y_true[valid_mask], y_pred[valid_mask]\n","    if len(y_true) == 0: return np.inf\n","    numerator = np.abs(y_true - y_pred); denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n","    mask = denominator != 0\n","    if np.sum(mask) == 0: return 0.0 if np.allclose(y_true, 0) and np.allclose(y_pred, 0) else np.inf\n","    valid_smape_values = numerator[mask] / denominator[mask]; smape = np.mean(valid_smape_values) * 100\n","    return smape if np.isfinite(smape) else np.finfo(np.float64).max\n","\n","def calculate_mase_scaling_factor(train_series):\n","    \"\"\"Menghitung faktor skala MASE (MAE dari in-sample one-step naive forecast).\"\"\"\n","    if len(train_series) \u003c 2: return 1.0\n","    train_series_pd = pd.Series(train_series)\n","    factor = np.mean(np.abs(train_series_pd.diff(1).dropna()))\n","    if pd.isna(factor) or factor \u003c 1e-9: return 1e-9\n","    return factor\n","\n","# ==============================================================================\n","# 5. Proses Utama\n","# ==============================================================================\n","\n","# 5.1 Memuat dan Memparsing Data\n","try:\n","    all_ids, all_start_times, all_series_data, data_freq, dataset_name = load_and_parse_data(\n","        SELECTED_DATASET_INDEX, DATASET_CONFIG\n","    )\n","    if all_ids is None: exit(f\"Gagal memuat/parsing Dataset {SELECTED_DATASET_INDEX}.\")\n","except Exception as e: print(f\"Error kritis load/parse: {e}\"); print(traceback.format_exc()); exit()\n","\n","# 5.2 Memilih, Membersihkan, dan Menganalisis Data Awal\n","try:\n","    selected_id, dataset_start_time, ts_raw = select_series(\n","        all_ids, all_start_times, all_series_data, SELECTED_SERIES_INDEX, dataset_name\n","    )\n","    if ts_raw is None or not isinstance(ts_raw, list): raise ValueError(\"Data mentah (ts_raw) tidak valid.\")\n","    if len(ts_raw) == 0: raise ValueError(\"Data mentah (ts_raw) kosong.\")\n","    ts_cleaned = handle_nan_values(ts_raw, method=NAN_IMPUTATION_METHOD)\n","    if not ts_cleaned: raise ValueError(\"Data kosong setelah cleaning NaN.\")\n","\n","    mase_scaling_factor = calculate_mase_scaling_factor(ts_cleaned)\n","    std_dev_original = np.std(np.array(ts_cleaned))\n","    if std_dev_original == 0 or np.isnan(std_dev_original):\n","        print(\"Peringatan: Standar deviasi data asli 0 atau NaN. Normalisasi residual diatur ke 1.\")\n","        std_dev_original = 1.0\n","    print(f\"Standar Deviasi Data Asli (untuk normalisasi residual): {std_dev_original:.4f}\")\n","    print(f\"Faktor Skala MASE (In-sample Naive MAE): {mase_scaling_factor:.4f}\")\n","\n","except (ValueError, IndexError) as e: print(f\"Error pilih/bersihkan seri: {e}\"); exit()\n","except Exception as e: print(f\"Error tak terduga pilih/bersihkan seri: {e}\"); print(traceback.format_exc()); exit()\n","\n","# 5.3 Normalisasi Data\n","print(\"\\nNormalisasi data (MinMaxScaler [0, 1])...\")\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","try:\n","    ts_cleaned_array = np.array(ts_cleaned).reshape(-1, 1)\n","    if np.all(ts_cleaned_array == ts_cleaned_array[0]): print(\"Warn: Data konstan setelah cleaning.\")\n","    ts_normalized = scaler.fit_transform(ts_cleaned_array).flatten()\n","    if np.isnan(ts_normalized).any(): print(\"Warn: NaN setelah normalisasi. Ganti dgn 0.\"); ts_normalized = np.nan_to_num(ts_normalized, nan=0.0)\n","except Exception as e: print(f\"Error normalisasi: {e}\"); print(traceback.format_exc()); exit()\n","\n","# 5.4 Perhitungan Training Adaptif \u0026 Pembuatan Fold CV\n","try:\n","    print(\"\\nMembuat fold CV dgn parameter tetap...\")\n","    folds = create_timeseries_cv_folds(\n","        ts_normalized, HORIZON, CV_STEP_SIZE, N_CROSSVALIDATION_FOLDS, data_freq, INPUT_WINDOW_SIZE\n","    )\n","    num_actual_folds = len(folds)\n","    train_window_length = len(folds[0][0])\n","    batches_per_epoch = int(np.ceil(train_window_length / BATCH_SIZE))\n","\n","    dynamic_max_steps = TARGET_EPOCHS_PER_FOLD * batches_per_epoch\n","\n","    final_max_steps = max(MIN_TRAINING_STEPS, min(dynamic_max_steps, MAX_TRAINING_BUDGET_STEPS))\n","\n","    final_val_freq = max(1, int(final_max_steps / 20))\n","    final_early_stop_patience_steps = EARLY_STOP_PATIENCE_VALIDATIONS * final_val_freq\n","\n","    print(\"\\n--- Konfigurasi Training Adaptif Dihitung ---\")\n","    print(f\"  Panjang Train per Fold: {train_window_length}\")\n","    print(f\"  Batch per Epoch       : {batches_per_epoch}\")\n","    print(f\"  Target Epochs         : {TARGET_EPOCHS_PER_FOLD} -\u003e (Target Steps: {dynamic_max_steps})\")\n","    print(f\"  Min/Max Steps Budget  : {MIN_TRAINING_STEPS} / {MAX_TRAINING_BUDGET_STEPS}\")\n","    print(f\"  FINAL Max Steps/Fold  : {final_max_steps}\")\n","    print(f\"  FINAL Freq. Validasi  : Setiap {final_val_freq} langkah\")\n","    print(f\"  FINAL Patience (steps): {final_early_stop_patience_steps} langkah\")\n","\n","except Exception as e: print(f\"Error buat fold CV atau hitung steps: {e}\"); print(traceback.format_exc()); exit()\n","\n","# 5.5 Definisi Fungsi Loss dan Loop Utama Eksperimen\n","loss_functions_to_test = {\n","    \"MSE\": MSE(), \"MAE\": MAE(), \"Huber\": HuberLoss(delta=1.0),\n","    \"MAPE\": MAPE(), \"SMAPE\": SMAPE(), \"Quantile0.1\": QuantileLoss(0.1),\n","    \"Quantile0.5\": QuantileLoss(0.5), \"Quantile0.9\": QuantileLoss(0.9)\n","}\n","\n","# \u003e\u003e\u003e PERBAIKAN: List untuk menyimpan semua hasil detail\n","all_detailed_results_dfs = []\n","overall_experiment_start_time = time.time()\n","model_name = MODEL_CONFIG[SELECTED_MODEL_INDEX]['name']\n","model_class = MODEL_CONFIG[SELECTED_MODEL_INDEX]['class']\n","\n","print(f\"\\n===== Memulai Eksperimen untuk Model: {model_name} =====\")\n","for loss_name, current_loss_function in loss_functions_to_test.items():\n","    print(f\"\\n===== Menjalankan Eksperimen untuk Loss: {loss_name} =====\")\n","\n","    # 5.5.1 Inisialisasi Model per Loss\n","    print(f\"\\nInisialisasi model {model_name} dgn loss {loss_name}...\")\n","    common_params = {\n","        'h': HORIZON, 'input_size': INPUT_WINDOW_SIZE, 'loss': current_loss_function,\n","        'max_steps': final_max_steps, 'batch_size': BATCH_SIZE, 'valid_loss': current_loss_function,\n","        'val_check_steps': final_val_freq,\n","        'early_stop_patience_steps': final_early_stop_patience_steps,\n","        'scaler_type': None, 'random_seed': seed\n","    }\n","    model_specific_params = {} # Tambahkan parameter spesifik model di sini jika perlu\n","    model_params = {**common_params, **model_specific_params}\n","    try:\n","        model_instance = model_class(**model_params)\n","        nf = NeuralForecast(models=[model_instance], freq=data_freq)\n","        print(f\"Model {model_name} \u0026 NeuralForecast diinisialisasi untuk loss {loss_name}.\")\n","    except Exception as e:\n","        print(f\"Error init model/NF untuk loss {loss_name}: {e}\"); print(traceback.format_exc())\n","        print(f\"!!! Melewati eksperimen untuk loss {loss_name} !!!\")\n","        continue\n","\n","    # 5.5.2 Loop Cross-Validation\n","    print(f\"\\nMemulai Cross-Validation untuk loss {loss_name}...\")\n","    fold_start_time_cv = time.time()\n","    current_loss_metrics_data = []\n","    if loss_name == list(loss_functions_to_test.keys())[-1]:\n","        all_predictions_denorm = []\n","        all_actuals_denorm = []\n","    successful_folds_current_loss = 0\n","\n","    for i, (train_fold_norm, test_fold_norm) in enumerate(folds):\n","        fold_num = i + 1\n","        print(f\"\\n--- Processing Fold {fold_num}/{num_actual_folds} (Loss: {loss_name}) ---\")\n","        fold_start_time_fold = time.time()\n","        if len(train_fold_norm) \u003c INPUT_WINDOW_SIZE: print(f\"Error: Train fold {fold_num} \u003c input size. Skip.\"); continue\n","        if len(test_fold_norm) == 0: print(f\"Error: Test fold {fold_num} kosong. Skip.\"); continue\n","        if np.isnan(train_fold_norm).any() or np.isnan(test_fold_norm).any(): print(f\"Warn: NaN di data fold {fold_num}.\")\n","\n","        fold_tracker = EmissionsTracker(measure_power_secs=1, log_level='warning', project_name=f\"{dataset_name}_{model_name}_{loss_name}_Fold_{fold_num}\")\n","        try: fold_tracker.start()\n","        except Exception as e: print(f\"Warn: Gagal start CodeCarbon: {e}\"); fold_tracker = None\n","\n","        # Persiapan Data Fold\n","        try:\n","            current_freq = nf.freq\n","            if not current_freq: raise ValueError(\"Freq tidak valid atau None.\")\n","            time_offset = pd.tseries.frequencies.to_offset(current_freq)\n","            train_start_index_global = i * CV_STEP_SIZE\n","            train_start_time_fold_ts = dataset_start_time + train_start_index_global * time_offset\n","            train_timestamps = pd.date_range(start=train_start_time_fold_ts, periods=len(train_fold_norm), freq=current_freq)\n","            test_start_time_fold_ts = train_timestamps[-1] + time_offset\n","            test_timestamps = pd.date_range(start=test_start_time_fold_ts, periods=len(test_fold_norm), freq=current_freq)\n","            train_df = prepare_dataframe_for_neuralforecast(train_fold_norm, selected_id, train_timestamps[0], current_freq)\n","            test_df_true = prepare_dataframe_for_neuralforecast(test_fold_norm, selected_id, test_timestamps[0], current_freq)\n","        except Exception as e:\n","            print(f\"Error prep data fold {fold_num}: {e}\"); print(traceback.format_exc()); print(\"Skip fold.\")\n","            if fold_tracker:\n","                try: fold_tracker.stop()\n","                except Exception as se: print(f\"Warn: Gagal stop tracker (error data prep): {se}\")\n","            continue\n","\n","        # Training Model\n","        print(f\"Training model fold {fold_num} (Loss: {loss_name}, Train: {len(train_df)}, Val: {VALIDATION_SIZE_FIT}, MaxSteps: {final_max_steps})...\")\n","        try:\n","            fit_output = nf.fit(df=train_df, val_size=VALIDATION_SIZE_FIT)\n","        except Exception as e:\n","            print(f\"Error training fold {fold_num} (Loss: {loss_name}): {e}\"); print(traceback.format_exc()); print(\"Skip fold.\")\n","            if fold_tracker:\n","                try: fold_tracker.stop()\n","                except Exception as se: print(f\"Warn: Gagal stop tracker (error training): {se}\")\n","            continue\n","\n","        # Prediksi\n","        print(f\"Prediksi horizon {HORIZON} (Loss: {loss_name})...\")\n","        try:\n","            forecast_df = nf.predict()\n","            if forecast_df is None or forecast_df.empty: raise ValueError(\"Hasil prediksi kosong.\")\n","            forecast_df = forecast_df.reset_index()\n","        except Exception as e:\n","            print(f\"Error prediksi fold {fold_num} (Loss: {loss_name}): {e}\"); print(traceback.format_exc()); print(\"Skip fold.\")\n","            if fold_tracker:\n","                try: fold_tracker.stop()\n","                except Exception as se: print(f\"Warn: Gagal stop tracker (error prediksi): {se}\")\n","            continue\n","\n","        # Evaluasi Fold\n","        evaluation_successful = False\n","        mae, mse, rmse, mape, smape, mase, mae_naive = (np.nan,) * 7\n","        mean_norm_res, std_norm_res, min_norm_res, max_norm_res, max_abs_norm_res = (np.nan,) * 5\n","        skew_res, kurt_res = np.nan, np.nan\n","        last_train_loss_fold, min_val_loss_fold, actual_steps_taken = np.nan, np.nan, np.nan\n","\n","        print(f\"Evaluasi fold {fold_num} (Loss: {loss_name})...\")\n","        try:\n","            y_true_normalized = test_df_true['y'].to_numpy()\n","            pred_col_name = model_name # Gunakan nama model dinamis\n","            if pred_col_name not in forecast_df.columns:\n","                possible_cols = [col for col in forecast_df.columns if col.startswith(model_name)]\n","                if possible_cols: pred_col_name = possible_cols[0]; print(f\"Warn: Guna kolom pred '{pred_col_name}'.\")\n","                else: raise KeyError(f\"Kolom pred '{model_name}'/* tdk ada. Kolom: {forecast_df.columns}\")\n","            y_pred_normalized = forecast_df[pred_col_name].to_numpy()\n","\n","            len_true, len_pred = len(y_true_normalized), len(y_pred_normalized)\n","            if len_pred != len_true:\n","                min_len = min(len_pred, len_true); print(f\"Warn: Panjang pred ({len_pred}) != aktual ({len_true}). Adjust ke {min_len}.\")\n","                if min_len == 0: raise ValueError(\"Data evaluasi 0 stlh adjust panjang.\")\n","                y_pred_normalized=y_pred_normalized[:min_len]; y_true_normalized=y_true_normalized[:min_len]; test_df_true=test_df_true.iloc[:min_len]\n","\n","            y_true_denorm = denormalize(y_true_normalized, scaler)\n","            y_pred_denorm = denormalize(y_pred_normalized, scaler)\n","            if np.isnan(y_true_denorm).any() or np.isnan(y_pred_denorm).any(): print(\"Warn: NaN setelah denorm.\")\n","\n","            if loss_name == list(loss_functions_to_test.keys())[-1]:\n","                 all_actuals_denorm.append(y_true_denorm); all_predictions_denorm.append(y_pred_denorm)\n","\n","            mae=mean_absolute_error(y_true_denorm, y_pred_denorm); mse=mean_squared_error(y_true_denorm, y_pred_denorm)\n","            rmse=np.sqrt(mse); mape=mean_absolute_percentage_error(y_true_denorm, y_pred_denorm)\n","            smape=symmetric_mean_absolute_percentage_error(y_true_denorm, y_pred_denorm)\n","\n","            last_train_val_norm = train_fold_norm[-1]\n","            last_train_val_denorm = denormalize(np.array([last_train_val_norm]), scaler)[0]\n","            y_pred_naive = np.full_like(y_true_denorm, last_train_val_denorm)\n","            mae_naive = mean_absolute_error(y_true_denorm, y_pred_naive)\n","            mase = mae / mase_scaling_factor if mase_scaling_factor \u003e 0 else np.inf\n","\n","            residuals = y_true_denorm - y_pred_denorm\n","            normalized_residuals = residuals / std_dev_original\n","            mean_norm_res=np.mean(normalized_residuals); std_norm_res=np.std(normalized_residuals)\n","            min_norm_res=np.min(normalized_residuals); max_norm_res=np.max(normalized_residuals)\n","            max_abs_norm_res=np.max(np.abs(normalized_residuals))\n","            skew_res=skew(residuals); kurt_res=kurtosis(residuals, fisher=True)\n","            if np.isnan(skew_res): skew_res = 0.0;\n","            if np.isnan(kurt_res): kurt_res = 0.0;\n","\n","            current_model_instance = nf.models[0]\n","            if hasattr(current_model_instance, 'trainer') and hasattr(current_model_instance.trainer, 'global_step'):\n","                actual_steps_taken = current_model_instance.trainer.global_step\n","            if hasattr(current_model_instance, 'losses_') and isinstance(current_model_instance.losses_, dict):\n","                if 'train_loss' in current_model_instance.losses_ and current_model_instance.losses_['train_loss']:\n","                     last_loss = current_model_instance.losses_['train_loss'][-1]\n","                     if isinstance(last_loss, (int, float)) and np.isfinite(last_loss): last_train_loss_fold = float(last_loss)\n","                if 'valid_loss' in current_model_instance.losses_ and current_model_instance.losses_['valid_loss']:\n","                     valid_numeric_losses = [float(l) for l in current_model_instance.losses_['valid_loss'] if isinstance(l,(int,float)) and np.isfinite(l)]\n","                     if valid_numeric_losses: min_val_loss_fold = min(valid_numeric_losses)\n","\n","            evaluation_successful = True\n","\n","        except Exception as eval_err:\n","            print(f\"Error evaluasi fold {fold_num} (Loss: {loss_name}): {eval_err}\"); print(traceback.format_exc()); print(\"Skip sisa evaluasi.\")\n","            pass\n","\n","        fold_end_time = time.time()\n","        fold_duration = fold_end_time - fold_start_time_fold\n","\n","        fold_emissions = 0.0\n","        if fold_tracker:\n","            try:\n","                fold_emissions_data = fold_tracker.stop()\n","                if isinstance(fold_emissions_data, (int, float)): fold_emissions = float(fold_emissions_data)\n","                elif isinstance(fold_emissions_data, dict) and 'emissions' in fold_emissions_data: fold_emissions = float(fold_emissions_data['emissions'])\n","                if not isinstance(fold_emissions, float) or not np.isfinite(fold_emissions): fold_emissions = 0.0\n","            except Exception as e: fold_emissions = 0.0\n","        else: fold_emissions = 0.0\n","\n","        if evaluation_successful:\n","            current_fold_metrics = (\n","                mae, mse, rmse, mape, smape, mase, mae_naive,\n","                mean_norm_res, std_norm_res, min_norm_res, max_norm_res, max_abs_norm_res,\n","                skew_res, kurt_res,\n","                last_train_loss_fold, min_val_loss_fold, actual_steps_taken, fold_duration, fold_emissions\n","            )\n","            current_loss_metrics_data.append(current_fold_metrics)\n","            successful_folds_current_loss += 1\n","\n","            print(f\"  Hasil Fold {fold_num}: [Sukses dievaluasi]\")\n","            print(f\"    Performa : MAE={mae:.4f}, MAE Naive={mae_naive:.4f}, MASE={mase:.4f}, RMSE={rmse:.4f}, MAPE={mape:.2f}%, sMAPE={smape:.2f}%\")\n","            print(f\"    Stabilitas: MeanNormRes={mean_norm_res:+.4f}, StdNormRes={std_norm_res:.4f}, Skew={skew_res:.3f}, Kurt={kurt_res:.3f}\")\n","            train_loss_str = f\"{last_train_loss_fold:.6f}\" if not np.isnan(last_train_loss_fold) else \"N/A\"\n","            val_loss_str = f\"{min_val_loss_fold:.6f}\" if not np.isnan(min_val_loss_fold) else \"N/A\"\n","            print(f\"    Konvergensi: TrainLoss(L)={train_loss_str}, ValLoss(Min)={val_loss_str}, ActualSteps={actual_steps_taken}\")\n","            print(f\"    Efisiensi: Durasi={fold_duration:.2f}s, CO2 Est.={fold_emissions:.6f} kg\")\n","        else:\n","            print(f\"  Hasil Fold {fold_num}: [Evaluasi Gagal]\")\n","            current_fold_metrics = (np.nan,) * 19\n","            current_loss_metrics_data.append(current_fold_metrics)\n","\n","\n","    # 5.5.3 Proses dan Simpan Hasil per Loss\n","    fold_end_time_cv = time.time()\n","    cv_duration_loss = fold_end_time_cv - fold_start_time_cv\n","    print(f\"\\nCross-Validation untuk loss '{loss_name}' selesai dalam {cv_duration_loss:.2f} detik. ({successful_folds_current_loss}/{num_actual_folds} fold sukses)\")\n","\n","    metric_names = [\n","        \"MAE\", \"MSE\", \"RMSE\", \"MAPE (%)\", \"sMAPE (%)\", \"MASE\", \"MAE Naive\",\n","        \"MeanNormRes\", \"StdNormRes\", \"MinNormRes\", \"MaxNormRes\", \"MaxAbsNormRes\",\n","        \"Skew Res\", \"Kurt Res\",\n","        \"Train Loss (L)\", \"Valid Loss (Min)\", \"Actual Steps\", \"Duration (s)\", \"CO2 (kg)\"\n","    ]\n","    results_df_loss = pd.DataFrame(current_loss_metrics_data, columns=metric_names)\n","    results_df_loss.index = range(1, num_actual_folds + 1)\n","    results_df_loss.index.name = \"Fold\"\n","\n","    if successful_folds_current_loss \u003e 0:\n","        mean_row = results_df_loss.mean(); std_row = results_df_loss.std()\n","        results_df_loss.loc['Rata-rata'] = mean_row\n","        results_df_loss.loc['Std Dev'] = std_row\n","\n","    # \u003e\u003e\u003e PERBAIKAN: Tambahkan kolom Loss Function dan simpan ke list \u003c\u003c\u003c\n","    results_df_loss['Loss Function'] = loss_name\n","    all_detailed_results_dfs.append(results_df_loss)\n","\n","\n","# ==============================================================================\n","# 6. Ringkasan Akhir dan Penyimpanan\n","# ==============================================================================\n","# 6.1 Ringkasan Konsol\n","overall_end_time_all = time.time()\n","overall_duration_all = overall_end_time_all - overall_experiment_start_time\n","print(f\"\\n=============================================================\")\n","print(f\" SEMUA EKSPERIMEN FUNGSI LOSS SELESAI\")\n","print(f\"=============================================================\")\n","print(f\" Dataset Diproses   : {dataset_name} (Indeks: {SELECTED_DATASET_INDEX})\")\n","print(f\" Seri Dipilih       : '{selected_id}' (Indeks Seri: {SELECTED_SERIES_INDEX})\")\n","print(f\" Model Dasar        : {model_name}\")\n","print(f\" Parameter Tetap    : Horizon={HORIZON}, InputWindow={INPUT_WINDOW_SIZE}\")\n","print(f\" Total Waktu Proses : {overall_duration_all:.2f} detik ({overall_duration_all/60:.2f} menit)\")\n","\n","# 6.2 Konsolidasi dan Penyimpanan Hasil\n","if GDRIVE_OUTPUT_FOLDER and all_detailed_results_dfs:\n","    # --- File 1: Hasil Detail Lengkap ---\n","    try:\n","        full_results_df = pd.concat(all_detailed_results_dfs).reset_index().set_index(['Loss Function', 'Fold'])\n","        dataset_name_safe = \"\".join(c if c.isalnum() else \"_\" for c in dataset_name)\n","        detailed_filename = f\"Hasil_Lengkap_{dataset_name_safe}_{model_name}.csv\"\n","        detailed_filepath = os.path.join(GDRIVE_OUTPUT_FOLDER, detailed_filename)\n","        print(f\"\\nMenyimpan hasil detail lengkap ke: {detailed_filepath}\")\n","        full_results_df.round(6).to_csv(detailed_filepath)\n","        saved_files.append(detailed_filepath)\n","        print(\"Berhasil disimpan.\")\n","    except Exception as e:\n","        print(f\"!!! GAGAL menyimpan file hasil detail: {e}\")\n","        print(traceback.format_exc())\n","\n","    # --- File 2: Ringkasan Perbandingan ---\n","    try:\n","        # Ambil hanya baris 'Rata-rata' dari setiap DataFrame\n","        summary_list = []\n","        for df_result in all_detailed_results_dfs:\n","            if 'Rata-rata' in df_result.index:\n","                summary_series = df_result.loc['Rata-rata']\n","                summary_series.name = df_result['Loss Function'].iloc[0] # Ambil nama loss\n","                summary_list.append(summary_series)\n","\n","        if summary_list:\n","            comparison_df = pd.DataFrame(summary_list)\n","            comparison_df.index.name = \"Loss Function\"\n","\n","            comparison_filename = f\"Ringkasan_Perbandingan_{dataset_name_safe}_{model_name}.csv\"\n","            comparison_filepath = os.path.join(GDRIVE_OUTPUT_FOLDER, comparison_filename)\n","            print(f\"\\nMenyimpan ringkasan perbandingan loss ke: {comparison_filepath}\")\n","            comparison_df.round(6).to_csv(comparison_filepath)\n","            saved_files.append(comparison_filepath)\n","            print(\"Berhasil disimpan.\")\n","\n","            # Tampilkan ringkasan di konsol juga\n","            print(\"\\n--- Ringkasan Perbandingan Rata-Rata Metrik Antar Loss Function ---\")\n","            pd.set_option('display.max_columns', None); pd.set_option('display.width', 120)\n","            print(comparison_df.round({\n","                \"MAPE (%)\": 2, \"sMAPE (%)\": 2, \"StdNormRes\": 4, \"Skew Res\": 3, \"Kurt Res\": 3,\n","                \"Valid Loss (Min)\": 6, \"Duration (s)\": 2, \"CO2 (kg)\": 6\n","            }).to_string())\n","        else:\n","            print(\"\\nTidak ada data rata-rata untuk membuat ringkasan perbandingan.\")\n","\n","    except Exception as e:\n","        print(f\"!!! GAGAL membuat atau menyimpan file ringkasan perbandingan: {e}\")\n","        print(traceback.format_exc())\n","\n","if saved_files:\n","    print(\"\\nFile Hasil yang Telah Disimpan:\")\n","    for file_path in saved_files: print(f\"- {file_path}\")\n","else:\n","    print(\"\\nTidak ada file hasil yang berhasil disimpan.\")\n","\n","\n","# ==============================================================================\n","# 7. Plotting (Opsional)\n","# ==============================================================================\n","print(\"\\nMencoba plot fold terakhir sukses (dari Loss Function terakhir)...\")\n","# (Logika plotting tetap sama, akan memplot hasil dari loss function terakhir yang dijalankan)\n","last_loss_results = all_detailed_results_dfs[-1] if all_detailed_results_dfs else pd.DataFrame()\n","successful_folds_last_loss = 0\n","if not last_loss_results.empty and 'Rata-rata' in last_loss_results.index:\n","     valid_fold_indices = [idx for idx in last_loss_results.index if isinstance(idx, int)]\n","     successful_folds_last_loss = len(valid_fold_indices)\n","\n","if all_actuals_denorm and all_predictions_denorm and successful_folds_last_loss \u003e 0:\n","    try:\n","        last_fold_actual = all_actuals_denorm[-1]; last_fold_pred = all_predictions_denorm[-1]\n","        last_successful_original_index = -1\n","        if successful_folds_last_loss \u003e 0:\n","            successful_indices_last_loss = [idx for idx in last_loss_results.index if isinstance(idx, int)]\n","            if successful_indices_last_loss:\n","                last_successful_fold_num = successful_indices_last_loss[-1]\n","                last_successful_original_index = last_successful_fold_num - 1\n","        if last_successful_original_index == -1: raise ValueError(\"Tdk dpt tentukan idx asli fold sukses terakhir.\")\n","        last_train_start_index_global = last_successful_original_index * CV_STEP_SIZE\n","        last_train_len = len(folds[last_successful_original_index][0])\n","        last_test_start_index_global = last_train_start_index_global + last_train_len\n","        current_freq = data_freq\n","        if not current_freq: raise ValueError(\"Freq tdk valid u/ plot.\")\n","        time_offset = pd.tseries.frequencies.to_offset(current_freq)\n","        last_test_start_time = dataset_start_time + last_test_start_index_global * time_offset\n","        last_test_timestamps = pd.date_range(start=last_test_start_time, periods=len(last_fold_actual), freq=current_freq)\n","        plt.figure(figsize=(15, 7))\n","        plt.plot(last_test_timestamps, last_fold_actual, label=f'Aktual (Fold Asli #{last_successful_original_index+1})', marker='.', linewidth=1)\n","        plt.plot(last_test_timestamps, last_fold_pred, label=f'Prediksi {model_name} (Loss Terakhir, Fold #{last_successful_original_index+1})', marker='.', linestyle='--', linewidth=1)\n","        plt.title(f'Prediksi vs Aktual - Fold Terakhir ({selected_id}) - Dataset: {dataset_name} - Loss: {list(loss_functions_to_test.keys())[-1]}')\n","        plt.xlabel('Timestamp'); plt.ylabel('Nilai (Skala Asli)'); plt.legend(); plt.grid(True); plt.xticks(rotation=30); plt.tight_layout(); plt.show()\n","    except Exception as e: print(f\"\\nError buat plot: {e}\"); print(traceback.format_exc())\n","else: print(\"\\nTidak ada data dari fold sukses (di loss terakhir) untuk diplot.\")\n","\n","print(\"\\n================ SEMUA EKSPERIMEN SELESAI ================\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMfQqsL3Td2H6uuzIajX7zV","gpuType":"T4","mount_file_id":"1I6BzyjmUI17VcAMIHQSGz5nZSjUdXcKc","name":"","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"02e9d394576a48cab2fd4e400bc7b432":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_11bff96530ba40bd9c8416563bf46d5a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a1e588b961244fb0bb2ffae908ecae32","value":0}},"09799285e5cf401990bfa1d0e4a460d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_32e3fe5e0b07478f9ebf9aa8990c31b9","IPY_MODEL_10554c531843442a98f454e8b2ef493e","IPY_MODEL_f0a2fec196ba4a12ac9ebbc9f3f84886"],"layout":"IPY_MODEL_f8d0824992f243a0950a6fe93b380446"}},"10554c531843442a98f454e8b2ef493e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b53d3651a2345a1abb73df5d513e6b8","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_43f6afd9b25c42a7ae6b751a7be0f96c","value":1}},"11bff96530ba40bd9c8416563bf46d5a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"234c08a50e4f43f6bc60420cb10848ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"32e3fe5e0b07478f9ebf9aa8990c31b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_742e9e91237a48a0b4caff633cfe43b5","placeholder":"​","style":"IPY_MODEL_60ea7f3bae564085a6d5daf7f2065dec","value":"Sanity Checking DataLoader 0: 100%"}},"43f6afd9b25c42a7ae6b751a7be0f96c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d0abfd69f4c4b149910dc3e474f1e08":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54c33ff788144f46975dc96b10f2c236":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b6a7c1d743447b5a20939619db33c7f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d0abfd69f4c4b149910dc3e474f1e08","placeholder":"​","style":"IPY_MODEL_234c08a50e4f43f6bc60420cb10848ce","value":" 0/1 [00:00\u0026lt;?, ?it/s, v_num=0, train_loss_step=0.0125, train_loss_epoch=0.0125]"}},"60ea7f3bae564085a6d5daf7f2065dec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"742e9e91237a48a0b4caff633cfe43b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b53d3651a2345a1abb73df5d513e6b8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"953a664c98bc4a09a2dcf8789f4474b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1e588b961244fb0bb2ffae908ecae32":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a9e9a6fd1e774e7ca5d6d199b0dc28a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54c33ff788144f46975dc96b10f2c236","placeholder":"​","style":"IPY_MODEL_e05ab8885d784874847ebcf63cefac7e","value":"Epoch 10:   0%"}},"b6bbecf95dde428abca9e07287cd829c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"dc561482acfb45a7a6be940a21154ff0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a9e9a6fd1e774e7ca5d6d199b0dc28a2","IPY_MODEL_02e9d394576a48cab2fd4e400bc7b432","IPY_MODEL_5b6a7c1d743447b5a20939619db33c7f"],"layout":"IPY_MODEL_b6bbecf95dde428abca9e07287cd829c"}},"ddaa19af467a4f5db7cb387de56baeeb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e05ab8885d784874847ebcf63cefac7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0a2fec196ba4a12ac9ebbc9f3f84886":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddaa19af467a4f5db7cb387de56baeeb","placeholder":"​","style":"IPY_MODEL_953a664c98bc4a09a2dcf8789f4474b2","value":" 1/1 [00:00\u0026lt;00:00, 26.18it/s]"}},"f8d0824992f243a0950a6fe93b380446":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}}}}},"nbformat":4,"nbformat_minor":0}